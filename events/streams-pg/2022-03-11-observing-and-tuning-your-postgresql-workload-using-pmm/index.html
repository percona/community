<!doctype html><html lang=en prefix="og: https://ogp.me/ns#"><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,minimum-scale=1"><title>Observing & Tuning Your PostgreSQL Workload Using PMM - â€¦ | Percona Community</title><meta name=description content="This Live stream is all about tuning your PostgreSQL workload with Percona Monitoring and Management (PMM). This is a part of Percona bi-weekly Live Stream â€¦"><link rel="shortcut icon" href=/superhero_huf5968b80ec9b386b36835fc6e8ce3113_170434_64x64_resize_lanczos_2.png type=image/png><link rel=canonical href=https://percona.community/events/streams-pg/2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm/><meta name=robots content="index,follow"><meta property="og:title" content="Observing & Tuning Your PostgreSQL Workload Using PMM - Community PostgreSQL Live Stream & Chat - March 11th"><meta property="og:description" content="This Live stream is all about tuning your PostgreSQL workload with Percona Monitoring and Management (PMM). This is a part of Percona bi-weekly Live Stream series dedicated to Postgres-related topics and deep talk about technology secrets and tricks."><meta property="og:type" content="article"><meta property="og:url" content="https://percona.community/events/streams-pg/2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm/"><meta name=image property="og:image" content="https://percona.community/events/streams-pg/pg-stream-week-2-march11-upd_hu0e86d1bd187a41bb8fcb7ec39995ef57_420960_1200x0_resize_q90_lanczos.jpg"><meta property="article:published_time" content="2022-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2022-03-11T00:00:00+00:00"><meta property="article:section" content="events"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://percona.community/events/streams-pg/pg-stream-week-2-march11-upd.jpg"><meta name=twitter:title content="Observing & Tuning Your PostgreSQL Workload Using PMM - Community PostgreSQL Live Stream & Chat - March 11th | Percona Community"><meta name=twitter:description content="This Live stream is all about tuning your PostgreSQL workload with Percona Monitoring and Management (PMM). This is a part of Percona bi-weekly Live Stream series dedicated to Postgres-related topics and deep talk about technology secrets and tricks."><meta name=twitter:site content="@Percona"><meta name=copyright content="Â© Percona Community. MySQL, InnoDB, MariaDB and MongoDB are trademarks of their respective owners.
"><style>@charset "UTF-8";html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after{content:none}q:before,q:after{content:none}table{border-collapse:collapse;border-spacing:0}html,body{font-family:Arial,sans-serif;scroll-behavior:smooth;text-rendering:optimizeSpeed;-webkit-font-smoothing:antialiased;line-height:1.5rem;font-feature-settings:"liga" 1;font-weight:400;font-size:18px}@media(min-width:900px){html,body{font-size:20px}}@media(min-width:1100px){html,body{font-size:22px}}@media(min-width:1800px){html,body{font-size:24px}}@media print{html,body{font-size:18px}}hr{border:0;height:2px;background-color:#fe7f2d;margin-top:1.5rem;margin-bottom:1.5rem}.text{position:relative}.text .meta{font-variant:small-caps}.text .meta .byline{display:none}.text .meta .line{display:inline-block}.text .meta .calendar{display:none}.text h1,.text h2,.text h3,.text h4,.text h5,.text h6{font-family:Georgia,palatino linotype,book antiqua,Palatino,times new roman,serif;font-weight:400;page-break-after:avoid}.text h1 a,.text h1 a:visited,.text h2 a,.text h2 a:visited,.text h3 a,.text h3 a:visited,.text h4 a,.text h4 a:visited,.text h5 a,.text h5 a:visited,.text h6 a,.text h6 a:visited{color:#333;text-decoration:none}.text p,.text ul,.text ol,.text figure,.text dl,.text hr{margin-top:.75rem;font-size:1rem;line-height:1.75rem}.text q{font-style:italic}.text ul ul{margin-top:0}.text p>code,.text ul>code,.text ol>code,.text dt>code,.text dd>code,.text li>code{color:#004966}.text audio{display:block;width:100%;height:1.5rem;outline:none}.text audio span{line-height:1.5rem}.text dl{overflow:auto;position:relative}.text dt{font-weight:700}.text dd{padding-left:2rem}.text dd+dt{margin-top:1.5rem}.text ul{list-style-type:disc;padding-left:1.5rem}.text ol{list-style-type:decimal;padding-left:1.5rem}.text p{overflow-wrap:break-word;word-wrap:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}.text h1{font-size:2rem;font-weight:400;line-height:2.5rem;margin-top:1.5rem;margin-bottom:0}@media screen and (min-width:760px){.text h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem}}@media screen and (min-width:900px){.text h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1200px){.text h1{font-size:3rem;line-height:4rem;margin-top:3rem}}.text h2{font-size:1.5rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h3{font-size:1.25rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h4{font-size:1rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem;font-size:1.1rem}.text h5{font-size:.8rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h6{font-size:.6rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text strong{font-weight:700}.text em{font-style:italic}.text code{font-family:lucida console,Monaco,monospace;speak:literal-punctuation}.text abbr,.text acronym{speak:spell-out}.text del{text-decoration:line-through}.text ins{text-decoration:none;font-style:italic}.text figure{position:relative;text-align:center}.text figure img{background:#fff;border-radius:.2rem;max-width:100%}.text figure table{min-width:75%;border-radius:.2rem;margin-left:auto;margin-right:auto;border:#004966;border-collapse:separate;border-spacing:0;background:#fff;margin-bottom:-.1em}.text figure table th,.text figure table td{padding-left:.75rem;padding-right:.75rem;line-height:1.3rem;border-left:.1rem solid #004966;border-top:.1rem solid #004966}.text figure table thead{background-color:#fff}.text figure table tr td:last-child,.text figure table tr th:last-child{border-right:.1rem solid #004966}.text figure table tbody:last-child tr:last-child td{border-bottom:.1rem solid #004966}.text figure table thead:first-child tr:first-child th:first-child,.text figure table thead:first-child tr:first-child td:first-child{border-top-left-radius:.2rem}.text figure table thead:first-child tr:first-child,.text figure table thead:first-child tr:first-child th:last-child,.text figure table thead:first-child tr:first-child td:last-child{border-top-right-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:first-child{border-bottom-left-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:last-child{border-bottom-right-radius:.2rem}.text figure ul{text-align:left}.text figure ul.tree{padding-top:1.5em;padding-bottom:1.5em;border-radius:.2rem;background:#fff}.text figure ul.tree li{list-style-type:none}.text figure ul.tree .tree__item--folder::before{content:"ðŸ“";margin-right:.2em}.text figure ul.tree .tree__item--file::before{content:"ðŸ“";margin-right:.2em}.text figure figcaption{font-style:italic}.text pre{margin-top:1.5rem;font-size:inherit;line-height:1.5rem;padding:1.5rem 1.5rem .75rem;white-space:pre;overflow-x:scroll;overflow-y:hidden;scrollbar-color:#cccccc transparent;scrollbar-width:.75rem}.text pre::-webkit-scrollbar{margin-top:-.75rem;height:.75rem}.text pre::-webkit-scrollbar-track{background:0 0}.text pre::-webkit-scrollbar-thumb{height:.1rem;border-radius:.2rem;background:#ccc}.text hr{border:0;height:.2rem;background:#004966;margin-top:1.3rem}.text .admonition{position:relative}.text .admonition>*{padding-left:2rem}.text .admonition>:first-child::before{display:block;text-align:left;position:absolute;top:0;left:0;width:.75rem;font-size:1rem;line-height:1.5rem;height:1.5rem}.text .admonition--tip>:first-child::before{content:"â˜›";color:#fefefe;font-size:1.5rem;line-height:1.5rem;height:1.5rem}.text .admonition--warning>:first-child::before{content:"âš ï¸";color:#fefefe}.text .admonition--info>:first-child::before{content:"â“˜";color:#004966;font-weight:700}@media print{.text h1{margin-left:auto;margin-right:auto;margin-top:9rem;width:80%;font-variant:small-caps;text-align:center}.text--simple h1{margin-top:0}.text .meta{page-break-after:always;text-align:center;font-size:1.2rem;margin-top:1.5rem}.text .meta a{text-decoration:none}.text .meta time{display:none}.text .body p:first-of-type{margin-top:0}.text pre{padding:0;font-size:inherit;line-height:1.5rem;overflow:hidden;white-space:pre-wrap;word-break:break-all}}.text .podcasts{margin-top:1.5rem;text-align:center}.text a.podcast{display:block;height:3rem}.text a.podcast img{width:50%;max-width:200px}@media screen and (min-width:768px){.text .podcasts{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));grid-gap:1.5rem}.text a.podcast img{width:100%}}.text .tombstone{opacity:0}html,body{min-height:100vh}body{text-align:center}@media screen{body{color:#333;background-color:#fefefe}}@media print{body{color:#000;background:#fff}}@media screen{body{display:flex;flex-direction:column}main.page{width:100%;flex:1}}header.page,main.page,footer.page{text-align:left}.page-main,.contentblock{max-width:1200px;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto}@media print{.page-main,.contentblock{padding:0}}@page{size:A4;padding:0;margin:3rem}.highlight{color:#004966}.nobr{white-space:nowrap}.aria-only{display:none}@media aural{.aria-only{display:block}}button{cursor:pointer;display:block;font-weight:700;margin:0 auto;border:.1rem solid #fe7f2d;background-color:#fe7f2d;font-size:1.2rem;border-radius:.9rem;padding:.6rem;transition:color .233s ease-in-out,background-color .233s ease-in-out}button:hover{background-color:#004966;border:.1rem solid #004966}blockquote{padding:0 2rem;margin:0 2rem}blockquote:before{display:block;content:"\1f5e8";font-size:2rem;padding:1rem 0 0;margin:0 0 -1.8rem -2rem;height:max-content;color:#fe7f2d;font-weight:700}a{color:#004966;font-weight:700;text-decoration-color:#004966;transition:color .233s ease-out,text-decoration-color .233s ease-out}a:hover,a:visited:hover{color:#fe7f2d;text-decoration-color:#fe7f2d}a:active{color:#fe7f2d;text-decoration-color:#fe7f2d}a:visited{color:#333}a.downloadbtn{text-decoration:none}html,body{font-family:Arial,sans-serif;scroll-behavior:smooth;text-rendering:optimizeSpeed;-webkit-font-smoothing:antialiased;line-height:1.5rem;font-feature-settings:"liga" 1;font-weight:400;font-size:18px}@media(min-width:900px){html,body{font-size:20px}}@media(min-width:1100px){html,body{font-size:22px}}@media(min-width:1800px){html,body{font-size:24px}}@media print{html,body{font-size:18px}}hr{border:0;height:2px;background-color:#fe7f2d;margin-top:1.5rem;margin-bottom:1.5rem}.text{position:relative}.text .meta{font-variant:small-caps}.text .meta .byline{display:none}.text .meta .line{display:inline-block}.text .meta .calendar{display:none}.text h1,.text h2,.text h3,.text h4,.text h5,.text h6{font-family:Georgia,palatino linotype,book antiqua,Palatino,times new roman,serif;font-weight:400;page-break-after:avoid}.text h1 a,.text h1 a:visited,.text h2 a,.text h2 a:visited,.text h3 a,.text h3 a:visited,.text h4 a,.text h4 a:visited,.text h5 a,.text h5 a:visited,.text h6 a,.text h6 a:visited{color:#333;text-decoration:none}.text p,.text ul,.text ol,.text figure,.text dl,.text hr{margin-top:.75rem;font-size:1rem;line-height:1.75rem}.text q{font-style:italic}.text ul ul{margin-top:0}.text p>code,.text ul>code,.text ol>code,.text dt>code,.text dd>code,.text li>code{color:#004966}.text audio{display:block;width:100%;height:1.5rem;outline:none}.text audio span{line-height:1.5rem}.text dl{overflow:auto;position:relative}.text dt{font-weight:700}.text dd{padding-left:2rem}.text dd+dt{margin-top:1.5rem}.text ul{list-style-type:disc;padding-left:1.5rem}.text ol{list-style-type:decimal;padding-left:1.5rem}.text p{overflow-wrap:break-word;word-wrap:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}.text h1{font-size:2rem;font-weight:400;line-height:2.5rem;margin-top:1.5rem;margin-bottom:0}@media screen and (min-width:760px){.text h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem}}@media screen and (min-width:900px){.text h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1200px){.text h1{font-size:3rem;line-height:4rem;margin-top:3rem}}.text h2{font-size:1.5rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h3{font-size:1.25rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h4{font-size:1rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem;font-size:1.1rem}.text h5{font-size:.8rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h6{font-size:.6rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text strong{font-weight:700}.text em{font-style:italic}.text code{font-family:lucida console,Monaco,monospace;speak:literal-punctuation}.text abbr,.text acronym{speak:spell-out}.text del{text-decoration:line-through}.text ins{text-decoration:none;font-style:italic}.text figure{position:relative;text-align:center}.text figure img{background:#fff;border-radius:.2rem;max-width:100%}.text figure table{min-width:75%;border-radius:.2rem;margin-left:auto;margin-right:auto;border:#004966;border-collapse:separate;border-spacing:0;background:#fff;margin-bottom:-.1em}.text figure table th,.text figure table td{padding-left:.75rem;padding-right:.75rem;line-height:1.3rem;border-left:.1rem solid #004966;border-top:.1rem solid #004966}.text figure table thead{background-color:#fff}.text figure table tr td:last-child,.text figure table tr th:last-child{border-right:.1rem solid #004966}.text figure table tbody:last-child tr:last-child td{border-bottom:.1rem solid #004966}.text figure table thead:first-child tr:first-child th:first-child,.text figure table thead:first-child tr:first-child td:first-child{border-top-left-radius:.2rem}.text figure table thead:first-child tr:first-child,.text figure table thead:first-child tr:first-child th:last-child,.text figure table thead:first-child tr:first-child td:last-child{border-top-right-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:first-child{border-bottom-left-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:last-child{border-bottom-right-radius:.2rem}.text figure ul{text-align:left}.text figure ul.tree{padding-top:1.5em;padding-bottom:1.5em;border-radius:.2rem;background:#fff}.text figure ul.tree li{list-style-type:none}.text figure ul.tree .tree__item--folder::before{content:"ðŸ“";margin-right:.2em}.text figure ul.tree .tree__item--file::before{content:"ðŸ“";margin-right:.2em}.text figure figcaption{font-style:italic}.text pre{margin-top:1.5rem;font-size:inherit;line-height:1.5rem;padding:1.5rem 1.5rem .75rem;white-space:pre;overflow-x:scroll;overflow-y:hidden;scrollbar-color:#cccccc transparent;scrollbar-width:.75rem}.text pre::-webkit-scrollbar{margin-top:-.75rem;height:.75rem}.text pre::-webkit-scrollbar-track{background:0 0}.text pre::-webkit-scrollbar-thumb{height:.1rem;border-radius:.2rem;background:#ccc}.text hr{border:0;height:.2rem;background:#004966;margin-top:1.3rem}.text .admonition{position:relative}.text .admonition>*{padding-left:2rem}.text .admonition>:first-child::before{display:block;text-align:left;position:absolute;top:0;left:0;width:.75rem;font-size:1rem;line-height:1.5rem;height:1.5rem}.text .admonition--tip>:first-child::before{content:"â˜›";color:#fefefe;font-size:1.5rem;line-height:1.5rem;height:1.5rem}.text .admonition--warning>:first-child::before{content:"âš ï¸";color:#fefefe}.text .admonition--info>:first-child::before{content:"â“˜";color:#004966;font-weight:700}@media print{.text h1{margin-left:auto;margin-right:auto;margin-top:9rem;width:80%;font-variant:small-caps;text-align:center}.text--simple h1{margin-top:0}.text .meta{page-break-after:always;text-align:center;font-size:1.2rem;margin-top:1.5rem}.text .meta a{text-decoration:none}.text .meta time{display:none}.text .body p:first-of-type{margin-top:0}.text pre{padding:0;font-size:inherit;line-height:1.5rem;overflow:hidden;white-space:pre-wrap;word-break:break-all}}.text .podcasts{margin-top:1.5rem;text-align:center}.text a.podcast{display:block;height:3rem}.text a.podcast img{width:50%;max-width:200px}@media screen and (min-width:768px){.text .podcasts{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));grid-gap:1.5rem}.text a.podcast img{width:100%}}.text .tombstone{opacity:0}.cmp-revoke-consent{bottom:0!important;left:0!important;position:relative!important;padding:0!important}.page-header a{display:block;text-decoration:none;height:2.6rem;line-height:2.6rem;border-top:.2rem solid transparent;border-bottom:.2rem solid transparent}.page-header a,.page-header a span{transition:color .233s ease-in-out,border-bottom .233s,margin-bottom .233s}.page-header a:hover{color:#004966;border-bottom:.2rem solid #004966}.page-header a:hover span{color:#004966}.page-header a img{height:2.6rem}.page-header a{color:#333;border-bottom:0 solid #fefefe}.page-header a:hover{color:#004966;border-bottom-color:#004966}.page-header a:visited{color:#333}.page-header a{display:block;border-bottom-width:.2rem;font-weight:400;font-size:.8rem}.page-header .logo{z-index:6;font-size:1.2rem;font-weight:700;font-variant:small-caps}.page-header .logo a{margin-top:.2rem;text-decoration:none;display:block;border:none}#show-mobile-menu{display:none}.mobile-toggle{display:none}@media screen and (max-width:999px){header.page{box-shadow:0 0 3px rgba(168,168,168,.5)}.logo a{position:absolute}.page-header{justify-content:space-between;z-index:1}.page-header a{border-bottom-width:0}.hamburger-bar{height:.2rem;background:#333;margin-top:.2rem;display:block}#show-mobile-menu+.mobile-toggle+.mobile-menu nav{background:#fefefe}#show-mobile-menu+.mobile-toggle .hamburger-bar{transform:translateY(0)rotate(0);opacity:1;transition:transform .233s ease-in-out,opacity .233s ease-in-out}#show-mobile-menu:checked+.mobile-toggle{position:fixed;right:1.5rem}#show-mobile-menu:checked+.mobile-toggle .hamburger-bar:nth-of-type(1){transform:translateY(.4rem)rotate(315deg)}#show-mobile-menu:checked+.mobile-toggle .hamburger-bar:nth-of-type(2){opacity:0}#show-mobile-menu:checked+.mobile-toggle .hamburger-bar:nth-of-type(3){transform:translateY(-.4rem)rotate(405deg)}.mobile-toggle{cursor:pointer;display:block;width:1.2rem;height:1rem;z-index:5;margin-top:.75rem}.mobile-toggle label,.mobile-toggle .hamburger-bar{cursor:pointer}.mobile-menu{overflow:hidden;position:fixed;top:0;left:0;right:0;bottom:100vh;opacity:0;background:rgba(0,0,0,.5);transition:opacity .233s ease-in-out,border-bottom-color .233s ease-in-out}#show-mobile-menu:checked+.mobile-toggle+.mobile-menu{opacity:1;bottom:0}.hide-mobile-menu{display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:2}.mobile-menu nav{box-sizing:border-box;max-height:100%;position:absolute;top:0;left:0;right:0;z-index:3;font-size:1rem;line-height:3rem;padding:3rem 1.5rem 1.5rem;display:flex;flex-direction:column}.mobile-menu nav header{display:flex;flex-direction:row;font-size:inherit;line-height:inherit}.mobile-menu nav header h1{margin-top:0;font-size:1.2rem;line-height:1.5rem;flex:1;font-variant:small-caps}.mobile-menu nav header .hide-mobile-menu-aria{display:block;z-index:10;height:1.5rem;width:1.5rem}.mobile-menu nav ul{overflow-y:scroll;flex:1;scrollbar-width:none;-ms-overflow-style:none}.mobile-menu nav ul::-webkit-scrollbar{width:0;background:0 0}.mobile-menu nav a{font-weight:700;font-size:1rem}.mobile-menu nav a:hover{border-bottom:0;margin-bottom:0}}@media screen and (min-width:1000px){.page-header .mobile-toggle{display:none}.page-header .mobile-menu{flex:1}.page-header nav{flex:1}.page-header nav header{display:none}.page-header ul{list-style-type:none}.page-header nav ul{display:flex;align-items:end;justify-content:flex-end;flex-direction:row}.page-header nav li{padding-left:1.5rem}.page-header nav li.active a{border-bottom-width:3px;border-bottom-color:#004966}}@media screen{.page-header{position:relative;max-width:1200px;margin-left:auto;margin-right:auto;padding-left:1.5rem;padding-right:1.5rem;height:3rem;display:flex;flex-direction:row}}@media print{header.page{display:none}}@media screen and (max-width:999px) and (prefers-reduced-motion:reduce){.mobile-menu{transition:none}#show-mobile-menu+.mobile-toggle .hamburger-bar{transition:none}}@media screen and (max-width:999px) and (prefers-reduced-transparency:reduce){.mobile-menu{background:gray}}.page-header nav li{position:relative}.page-header nav li:hover ul.submenu__list{display:flex;z-index:999}.page-header nav ul.submenu__list{display:none;flex-direction:column;background:#fff;position:absolute;left:0}.page-header nav ul.submenu__list li{padding:0 20px}.page-header nav ul.submenu__list li a{white-space:nowrap}@media(max-width:768px){.page-header nav ul.submenu__list{display:flex;position:static}.page-header nav ul.submenu__list li a{font-weight:400}}html,body{font-family:Arial,sans-serif;scroll-behavior:smooth;text-rendering:optimizeSpeed;-webkit-font-smoothing:antialiased;line-height:1.5rem;font-feature-settings:"liga" 1;font-weight:400;font-size:18px}@media(min-width:900px){html,body{font-size:20px}}@media(min-width:1100px){html,body{font-size:22px}}@media(min-width:1800px){html,body{font-size:24px}}@media print{html,body{font-size:18px}}hr{border:0;height:2px;background-color:#fe7f2d;margin-top:1.5rem;margin-bottom:1.5rem}.text{position:relative}.text .meta{font-variant:small-caps}.text .meta .byline{display:none}.text .meta .line{display:inline-block}.text .meta .calendar{display:none}.text h1,.text h2,.text h3,.text h4,.text h5,.text h6{font-family:Georgia,palatino linotype,book antiqua,Palatino,times new roman,serif;font-weight:400;page-break-after:avoid}.text h1 a,.text h1 a:visited,.text h2 a,.text h2 a:visited,.text h3 a,.text h3 a:visited,.text h4 a,.text h4 a:visited,.text h5 a,.text h5 a:visited,.text h6 a,.text h6 a:visited{color:#333;text-decoration:none}.text p,.text ul,.text ol,.text figure,.text dl,.text hr{margin-top:.75rem;font-size:1rem;line-height:1.75rem}.text q{font-style:italic}.text ul ul{margin-top:0}.text p>code,.text ul>code,.text ol>code,.text dt>code,.text dd>code,.text li>code{color:#004966}.text audio{display:block;width:100%;height:1.5rem;outline:none}.text audio span{line-height:1.5rem}.text dl{overflow:auto;position:relative}.text dt{font-weight:700}.text dd{padding-left:2rem}.text dd+dt{margin-top:1.5rem}.text ul{list-style-type:disc;padding-left:1.5rem}.text ol{list-style-type:decimal;padding-left:1.5rem}.text p{overflow-wrap:break-word;word-wrap:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}.text h1{font-size:2rem;font-weight:400;line-height:2.5rem;margin-top:1.5rem;margin-bottom:0}@media screen and (min-width:760px){.text h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem}}@media screen and (min-width:900px){.text h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1200px){.text h1{font-size:3rem;line-height:4rem;margin-top:3rem}}.text h2{font-size:1.5rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h3{font-size:1.25rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h4{font-size:1rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem;font-size:1.1rem}.text h5{font-size:.8rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h6{font-size:.6rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text strong{font-weight:700}.text em{font-style:italic}.text code{font-family:lucida console,Monaco,monospace;speak:literal-punctuation}.text abbr,.text acronym{speak:spell-out}.text del{text-decoration:line-through}.text ins{text-decoration:none;font-style:italic}.text figure{position:relative;text-align:center}.text figure img{background:#fff;border-radius:.2rem;max-width:100%}.text figure table{min-width:75%;border-radius:.2rem;margin-left:auto;margin-right:auto;border:#004966;border-collapse:separate;border-spacing:0;background:#fff;margin-bottom:-.1em}.text figure table th,.text figure table td{padding-left:.75rem;padding-right:.75rem;line-height:1.3rem;border-left:.1rem solid #004966;border-top:.1rem solid #004966}.text figure table thead{background-color:#fff}.text figure table tr td:last-child,.text figure table tr th:last-child{border-right:.1rem solid #004966}.text figure table tbody:last-child tr:last-child td{border-bottom:.1rem solid #004966}.text figure table thead:first-child tr:first-child th:first-child,.text figure table thead:first-child tr:first-child td:first-child{border-top-left-radius:.2rem}.text figure table thead:first-child tr:first-child,.text figure table thead:first-child tr:first-child th:last-child,.text figure table thead:first-child tr:first-child td:last-child{border-top-right-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:first-child{border-bottom-left-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:last-child{border-bottom-right-radius:.2rem}.text figure ul{text-align:left}.text figure ul.tree{padding-top:1.5em;padding-bottom:1.5em;border-radius:.2rem;background:#fff}.text figure ul.tree li{list-style-type:none}.text figure ul.tree .tree__item--folder::before{content:"ðŸ“";margin-right:.2em}.text figure ul.tree .tree__item--file::before{content:"ðŸ“";margin-right:.2em}.text figure figcaption{font-style:italic}.text pre{margin-top:1.5rem;font-size:inherit;line-height:1.5rem;padding:1.5rem 1.5rem .75rem;white-space:pre;overflow-x:scroll;overflow-y:hidden;scrollbar-color:#cccccc transparent;scrollbar-width:.75rem}.text pre::-webkit-scrollbar{margin-top:-.75rem;height:.75rem}.text pre::-webkit-scrollbar-track{background:0 0}.text pre::-webkit-scrollbar-thumb{height:.1rem;border-radius:.2rem;background:#ccc}.text hr{border:0;height:.2rem;background:#004966;margin-top:1.3rem}.text .admonition{position:relative}.text .admonition>*{padding-left:2rem}.text .admonition>:first-child::before{display:block;text-align:left;position:absolute;top:0;left:0;width:.75rem;font-size:1rem;line-height:1.5rem;height:1.5rem}.text .admonition--tip>:first-child::before{content:"â˜›";color:#fefefe;font-size:1.5rem;line-height:1.5rem;height:1.5rem}.text .admonition--warning>:first-child::before{content:"âš ï¸";color:#fefefe}.text .admonition--info>:first-child::before{content:"â“˜";color:#004966;font-weight:700}@media print{.text h1{margin-left:auto;margin-right:auto;margin-top:9rem;width:80%;font-variant:small-caps;text-align:center}.text--simple h1{margin-top:0}.text .meta{page-break-after:always;text-align:center;font-size:1.2rem;margin-top:1.5rem}.text .meta a{text-decoration:none}.text .meta time{display:none}.text .body p:first-of-type{margin-top:0}.text pre{padding:0;font-size:inherit;line-height:1.5rem;overflow:hidden;white-space:pre-wrap;word-break:break-all}}.text .podcasts{margin-top:1.5rem;text-align:center}.text a.podcast{display:block;height:3rem}.text a.podcast img{width:50%;max-width:200px}@media screen and (min-width:768px){.text .podcasts{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));grid-gap:1.5rem}.text a.podcast img{width:100%}}.text .tombstone{opacity:0}html,body{min-height:100vh}body{text-align:center}@media screen{body{color:#333;background-color:#fefefe}}@media print{body{color:#000;background:#fff}}@media screen{body{display:flex;flex-direction:column}main.page{width:100%;flex:1}}header.page,main.page,footer.page{text-align:left}.page-main,.contentblock{max-width:1200px;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto}@media print{.page-main,.contentblock{padding:0}}@page{size:A4;padding:0;margin:3rem}.highlight{color:#004966}.nobr{white-space:nowrap}.aria-only{display:none}@media aural{.aria-only{display:block}}.cmp-revoke-consent{bottom:0!important;left:0!important;position:relative!important;padding:0!important}footer.page{margin-top:1.5rem}footer.page h2{font-size:1rem}@media not print{footer.page{margin-top:1.5rem/2;padding-top:1.5rem/2;padding-bottom:1.5rem/2}.page-footer{max-width:1200px;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto;font-size:.7rem;line-height:1.5rem}}@media not print{.page-footer ul{margin-bottom:1.5rem}.page-footer a{display:block;text-decoration:none;height:2.6rem;line-height:2.6rem;border-top:.2rem solid transparent;border-bottom:.2rem solid transparent}.page-footer a,.page-footer a span{transition:color .233s ease-in-out,border-bottom .233s,margin-bottom .233s}.page-footer a:hover{color:#004966;border-bottom:.2rem solid #004966}.page-footer a:hover span{color:#004966}.page-footer a img{height:2.6rem}.page-footer nav ul{display:flex;flex-direction:row;align-items:center;justify-content:center;margin-bottom:0;height:1.5rem;line-height:1.5rem}.page-footer nav ul a{height:1.3rem;line-height:1.3rem;text-align:center;display:block;padding-left:.75rem;padding-right:.75rem;border-bottom:.2rem solid transparent}}@media print{footer.page nav{display:none}footer.page aside{border-top:.2rem solid #333;margin-top:.65rem;padding-top:.65rem}}footer.page{border-top:2px solid #fe7f2d;margin-top:-2px}footer.page aside{text-align:center;font-size:.6rem;line-height:1.5rem;color:#333}@media screen{footer.page{background-color:#fefefe}.page-footer a{color:#333;border-bottom:0 solid #fefefe}.page-footer a:hover{color:#004966;border-bottom-color:#004966}.page-footer a:visited{color:#333}}.youtubeList__container{display:flex;flex-direction:row;justify-content:flex-start;margin-bottom:1.5rem}.youtubeList__container .youtubeList__img{margin-right:1rem}.grid{display:grid;gap:1.5rem 1.5rem;font-size:.9rem;margin-bottom:1.5rem;margin-top:1.5rem}.grid--2{grid-template-columns:1fr 1fr}.grid--3{grid-template-columns:1fr 1fr 1fr}.grid--4{grid-template-columns:1fr 1fr 1fr 1fr}.grid img{max-width:100%}.grid h2{font-size:1rem;line-height:1.25rem;margin-bottom:.75rem}.grid p{font-size:.75rem;line-height:1rem;font-weight:400;margin-bottom:.75rem}.grid ul{margin-left:1rem}.grid ul li{list-style-type:disc}@media(max-width:768px){.grid__item{margin-bottom:1rem}.grid--2{display:block}}@media(max-width:1200px){.grid--3{display:block}}@media(max-width:1200px){.grid--4{display:block}}.grid-links{display:grid;gap:1.5rem 1.5rem;font-size:.9rem;margin-bottom:1.5rem;margin-top:1.5rem}.grid-links--2{grid-template-columns:1fr 1fr}.grid-links--3{grid-template-columns:1fr 1fr 1fr}.grid-links--4{grid-template-columns:1fr 1fr 1fr 1fr}.grid-links img{max-width:100%}.grid-links h2{font-size:1.2rem;margin-bottom:.75rem}.grid-links p{margin-bottom:.75rem}.grid-links ul{margin-left:1rem}.grid-links ul li{list-style-type:disc}@media(max-width:768px){.grid-links--2{display:block}}@media(max-width:1000px){.grid-links--3{display:block}}@media(max-width:1000px){.grid-links--4{display:block}}.post--preview{display:block;border-radius:.3rem;padding-bottom:.375rem}.post--preview img{margin-bottom:.375rem}.post--preview h2,.post--preview h3,.post--preview p{padding-left:.5rem;padding-right:.5rem}.post--preview :last-child{margin-bottom:0}.post--preview:hover{box-shadow:0 0 1.5rem #7f7f7f}a.post--preview,a.post--preview *{text-decoration:none;color:#333}a.post--preview .readmore,a.post--preview * .readmore{font-weight:700}.recommendations{margin-top:2rem}.text .grid h2{font-size:1rem;line-height:1.25rem;margin-bottom:.75rem;margin-top:0;font-weight:700}.text .post__content p{font-size:.75rem;line-height:1rem;font-weight:400}.text .post__content .readmore{font-weight:700}.comments .comment{padding:2em;background:#f5f5f5;border-radius:3%;margin-bottom:2em}.comments .info{margin-bottom:2em}.comments .info a{font-size:1.2em}.comments .info span{float:right;color:#9c98a8;font-style:italic}.banner{background-color:#004966;background-position:50%;background-repeat:no-repeat;background-size:cover;color:#fff}.banner h1{margin-bottom:1.5rem}.banner .banner__content{max-width:1200px;padding:1.5rem;margin-left:auto;margin-right:auto}.banner .banner__grid{display:grid;gap:1.5rem 1.5rem;grid-template-columns:9fr 3fr;margin-top:1.5rem;margin-bottom:1.5rem}@media screen and (max-width:1000px){.banner .banner__grid{display:block}}.banner .banner__text{font-size:1.2rem;background:rgba(0,0,0,.7);padding:1.5rem;border-radius:.75rem}.banner .banner__text h1{font-size:1.5rem}.banner .banner__text p{margin-top:1.5rem}.banner .banner__text p:first-child{margin-top:0}.banner .banner__text small{font-size:.8rem}.banner .banner__text.opaque{background:rgba(0,0,0,.95)}.banner .banner__text small{font-size:.8rem}.banner .banner__icon{float:right;text-align:center}@media screen and (max-width:1000px){.banner .banner__icon{display:none}}.banner a{color:#fff;font-weight:700;border-bottom:2px solid #004966;text-decoration:none;transition:color .233s ease-out,border-bottom-color .233s ease-out}.banner a:hover,.banner a:visited:hover{color:#fff;border-bottom-color:#fe7f2d}table{border:1px solid #ccc;border-collapse:collapse;margin-top:2rem;padding:0;width:100%;table-layout:fixed}table caption{font-size:1.5em;margin:.5em 0 .75em}table tr{background-color:#f8f8f8;border:1px solid #ddd;padding:.35em}table th,table td{padding:.625em;text-align:left}table th{font-size:1em;letter-spacing:.1em;text-transform:uppercase}@media screen and (max-width:600px){table{border:0}table caption{font-size:1.3em}table thead{border:none;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}table tr{border-bottom:3px solid #ddd;display:block;margin-bottom:.625em}table td{border-bottom:1px solid #ddd;display:block;font-size:1em;text-align:left}table td::before{content:attr(data-label);float:left;font-weight:700;text-transform:uppercase}table td:last-child{border-bottom:0}}.hero{background-color:#004966;background-position:50%;background-repeat:no-repeat;background-size:cover;color:#fff}.hero h1{margin-bottom:1.5rem;line-height:2.5rem}.hero h2{margin-top:1rem;margin-bottom:1rem}.hero h2{margin-top:1rem;margin-bottom:1rem}.hero a{color:#fff;font-weight:700;border-bottom:2px solid #004966;text-decoration:none;transition:color .233s ease-out,border-bottom-color .233s ease-out}.hero a:hover,.hero a:visited:hover{color:#fff;border-bottom-color:#fe7f2d}@media screen and (max-width:767px){.hero figure{position:relative;text-align:center}}.hero img{float:left;margin-right:1.5rem;max-width:100%}@media screen and (max-width:767px){.hero img{float:none;margin-left:auto;margin-right:auto;display:block}}.hero h2+p+figure img{position:relative;top:-3rem}@media screen and (max-width:767px){.hero h2+p+figure img{top:0}}.hero:nth-child(4n+2),.hero:nth-child(4n+4){background-color:initial;color:#333}.hero:nth-child(4n+2) img,.hero:nth-child(4n+4) img{float:right;margin-right:0;margin-left:1.5rem}@media screen and (max-width:767px){.hero:nth-child(4n+2) img,.hero:nth-child(4n+4) img{float:none;margin-left:auto;margin-right:auto}}.hero:nth-child(4n+2) a,.hero:nth-child(4n+4) a{color:#333}.hero:nth-child(4n+2) a:hover,.hero:nth-child(4n+4) a:hover{color:#333}.hero:nth-child(4n+3){background-color:#fe7f2d}.hero--bio .contentblock{text-align:center}.hero--bio .contentblock img{text-align:center;float:none;margin-bottom:1.5rem}.hero--bio .contentblock p{text-align:left}@media(min-width:769px){.hero--bio .contentblock{min-height:240px;text-align:left}.hero--bio .contentblock img{float:right;margin-left:1.5rem;margin-bottom:0}}.contentblock{padding-top:2rem;padding-bottom:2rem}.herotext{font-size:1.2rem;background:rgba(0,0,0,.7);padding:1.5rem;border-radius:.75rem}.herotext p{margin-top:1.5rem}.herotext p:first-child{margin-top:0}.herotext small{font-size:.8rem}.herotext.opaque{background:rgba(0,0,0,.95)}html,body{font-family:Arial,sans-serif;scroll-behavior:smooth;text-rendering:optimizeSpeed;-webkit-font-smoothing:antialiased;line-height:1.5rem;font-feature-settings:"liga" 1;font-weight:400;font-size:18px}@media(min-width:900px){html,body{font-size:20px}}@media(min-width:1100px){html,body{font-size:22px}}@media(min-width:1800px){html,body{font-size:24px}}@media print{html,body{font-size:18px}}hr{border:0;height:2px;background-color:#fe7f2d;margin-top:1.5rem;margin-bottom:1.5rem}.text{position:relative}.text .meta{font-variant:small-caps}.text .meta .byline{display:none}.text .meta .line{display:inline-block}.text .meta .calendar{display:none}.text h1,.text h2,.text h3,.text h4,.text h5,.text h6{font-family:Georgia,palatino linotype,book antiqua,Palatino,times new roman,serif;font-weight:400;page-break-after:avoid}.text h1 a,.text h1 a:visited,.text h2 a,.text h2 a:visited,.text h3 a,.text h3 a:visited,.text h4 a,.text h4 a:visited,.text h5 a,.text h5 a:visited,.text h6 a,.text h6 a:visited{color:#333;text-decoration:none}.text p,.text ul,.text ol,.text figure,.text dl,.text hr{margin-top:.75rem;font-size:1rem;line-height:1.75rem}.text q{font-style:italic}.text ul ul{margin-top:0}.text p>code,.text ul>code,.text ol>code,.text dt>code,.text dd>code,.text li>code{color:#004966}.text audio{display:block;width:100%;height:1.5rem;outline:none}.text audio span{line-height:1.5rem}.text dl{overflow:auto;position:relative}.text dt{font-weight:700}.text dd{padding-left:2rem}.text dd+dt{margin-top:1.5rem}.text ul{list-style-type:disc;padding-left:1.5rem}.text ol{list-style-type:decimal;padding-left:1.5rem}.text p{overflow-wrap:break-word;word-wrap:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}.text h1{font-size:2rem;font-weight:400;line-height:2.5rem;margin-top:1.5rem;margin-bottom:0}@media screen and (min-width:760px){.text h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem}}@media screen and (min-width:900px){.text h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1200px){.text h1{font-size:3rem;line-height:4rem;margin-top:3rem}}.text h2{font-size:1.5rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h3{font-size:1.25rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h4{font-size:1rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem;font-size:1.1rem}.text h5{font-size:.8rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h6{font-size:.6rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text strong{font-weight:700}.text em{font-style:italic}.text code{font-family:lucida console,Monaco,monospace;speak:literal-punctuation}.text abbr,.text acronym{speak:spell-out}.text del{text-decoration:line-through}.text ins{text-decoration:none;font-style:italic}.text figure{position:relative;text-align:center}.text figure img{background:#fff;border-radius:.2rem;max-width:100%}.text figure table{min-width:75%;border-radius:.2rem;margin-left:auto;margin-right:auto;border:#004966;border-collapse:separate;border-spacing:0;background:#fff;margin-bottom:-.1em}.text figure table th,.text figure table td{padding-left:.75rem;padding-right:.75rem;line-height:1.3rem;border-left:.1rem solid #004966;border-top:.1rem solid #004966}.text figure table thead{background-color:#fff}.text figure table tr td:last-child,.text figure table tr th:last-child{border-right:.1rem solid #004966}.text figure table tbody:last-child tr:last-child td{border-bottom:.1rem solid #004966}.text figure table thead:first-child tr:first-child th:first-child,.text figure table thead:first-child tr:first-child td:first-child{border-top-left-radius:.2rem}.text figure table thead:first-child tr:first-child,.text figure table thead:first-child tr:first-child th:last-child,.text figure table thead:first-child tr:first-child td:last-child{border-top-right-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:first-child{border-bottom-left-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:last-child{border-bottom-right-radius:.2rem}.text figure ul{text-align:left}.text figure ul.tree{padding-top:1.5em;padding-bottom:1.5em;border-radius:.2rem;background:#fff}.text figure ul.tree li{list-style-type:none}.text figure ul.tree .tree__item--folder::before{content:"ðŸ“";margin-right:.2em}.text figure ul.tree .tree__item--file::before{content:"ðŸ“";margin-right:.2em}.text figure figcaption{font-style:italic}.text pre{margin-top:1.5rem;font-size:inherit;line-height:1.5rem;padding:1.5rem 1.5rem .75rem;white-space:pre;overflow-x:scroll;overflow-y:hidden;scrollbar-color:#cccccc transparent;scrollbar-width:.75rem}.text pre::-webkit-scrollbar{margin-top:-.75rem;height:.75rem}.text pre::-webkit-scrollbar-track{background:0 0}.text pre::-webkit-scrollbar-thumb{height:.1rem;border-radius:.2rem;background:#ccc}.text hr{border:0;height:.2rem;background:#004966;margin-top:1.3rem}.text .admonition{position:relative}.text .admonition>*{padding-left:2rem}.text .admonition>:first-child::before{display:block;text-align:left;position:absolute;top:0;left:0;width:.75rem;font-size:1rem;line-height:1.5rem;height:1.5rem}.text .admonition--tip>:first-child::before{content:"â˜›";color:#fefefe;font-size:1.5rem;line-height:1.5rem;height:1.5rem}.text .admonition--warning>:first-child::before{content:"âš ï¸";color:#fefefe}.text .admonition--info>:first-child::before{content:"â“˜";color:#004966;font-weight:700}@media print{.text h1{margin-left:auto;margin-right:auto;margin-top:9rem;width:80%;font-variant:small-caps;text-align:center}.text--simple h1{margin-top:0}.text .meta{page-break-after:always;text-align:center;font-size:1.2rem;margin-top:1.5rem}.text .meta a{text-decoration:none}.text .meta time{display:none}.text .body p:first-of-type{margin-top:0}.text pre{padding:0;font-size:inherit;line-height:1.5rem;overflow:hidden;white-space:pre-wrap;word-break:break-all}}.text .podcasts{margin-top:1.5rem;text-align:center}.text a.podcast{display:block;height:3rem}.text a.podcast img{width:50%;max-width:200px}@media screen and (min-width:768px){.text .podcasts{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));grid-gap:1.5rem}.text a.podcast img{width:100%}}.text .tombstone{opacity:0}.paginator{margin-top:1.5rem;text-align:center}.paginator a{display:inline-block;text-decoration:none;border-radius:3px;color:#333;border:1px solid #333;transition:.2s background-color ease-in-out,.2s color ease-in-out,.2s border-color ease-in-out;height:1.5rem;width:1.5rem;line-height:1.5rem;font-size:1rem;margin-left:.2rem;margin-right:.2rem}.paginator a:hover,.paginator a.active{background:#004966;color:#fefefe;border:1px solid #004966}@media screen and (min-width:768px){.paginator a{height:1.5rem;width:1.5rem;line-height:1.5rem;font-size:1rem}}@media screen and (min-width:900px){.paginator a{height:3rem;width:3rem;line-height:3rem;font-size:1.2rem}}html,body{font-family:Arial,sans-serif;scroll-behavior:smooth;text-rendering:optimizeSpeed;-webkit-font-smoothing:antialiased;line-height:1.5rem;font-feature-settings:"liga" 1;font-weight:400;font-size:18px}@media(min-width:900px){html,body{font-size:20px}}@media(min-width:1100px){html,body{font-size:22px}}@media(min-width:1800px){html,body{font-size:24px}}@media print{html,body{font-size:18px}}hr{border:0;height:2px;background-color:#fe7f2d;margin-top:1.5rem;margin-bottom:1.5rem}.text{position:relative}.text .meta{font-variant:small-caps}.text .meta .byline{display:none}.text .meta .line{display:inline-block}.text .meta .calendar{display:none}.text h1,.text h2,.text h3,.text h4,.text h5,.text h6{font-family:Georgia,palatino linotype,book antiqua,Palatino,times new roman,serif;font-weight:400;page-break-after:avoid}.text h1 a,.text h1 a:visited,.text h2 a,.text h2 a:visited,.text h3 a,.text h3 a:visited,.text h4 a,.text h4 a:visited,.text h5 a,.text h5 a:visited,.text h6 a,.text h6 a:visited{color:#333;text-decoration:none}.text p,.text ul,.text ol,.text figure,.text dl,.text hr{margin-top:.75rem;font-size:1rem;line-height:1.75rem}.text q{font-style:italic}.text ul ul{margin-top:0}.text p>code,.text ul>code,.text ol>code,.text dt>code,.text dd>code,.text li>code{color:#004966}.text audio{display:block;width:100%;height:1.5rem;outline:none}.text audio span{line-height:1.5rem}.text dl{overflow:auto;position:relative}.text dt{font-weight:700}.text dd{padding-left:2rem}.text dd+dt{margin-top:1.5rem}.text ul{list-style-type:disc;padding-left:1.5rem}.text ol{list-style-type:decimal;padding-left:1.5rem}.text p{overflow-wrap:break-word;word-wrap:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}.text h1{font-size:2rem;font-weight:400;line-height:2.5rem;margin-top:1.5rem;margin-bottom:0}@media screen and (min-width:760px){.text h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem}}@media screen and (min-width:900px){.text h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1200px){.text h1{font-size:3rem;line-height:4rem;margin-top:3rem}}.text h2{font-size:1.5rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h3{font-size:1.25rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h4{font-size:1rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem;font-size:1.1rem}.text h5{font-size:.8rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h6{font-size:.6rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text strong{font-weight:700}.text em{font-style:italic}.text code{font-family:lucida console,Monaco,monospace;speak:literal-punctuation}.text abbr,.text acronym{speak:spell-out}.text del{text-decoration:line-through}.text ins{text-decoration:none;font-style:italic}.text figure{position:relative;text-align:center}.text figure img{background:#fff;border-radius:.2rem;max-width:100%}.text figure table{min-width:75%;border-radius:.2rem;margin-left:auto;margin-right:auto;border:#004966;border-collapse:separate;border-spacing:0;background:#fff;margin-bottom:-.1em}.text figure table th,.text figure table td{padding-left:.75rem;padding-right:.75rem;line-height:1.3rem;border-left:.1rem solid #004966;border-top:.1rem solid #004966}.text figure table thead{background-color:#fff}.text figure table tr td:last-child,.text figure table tr th:last-child{border-right:.1rem solid #004966}.text figure table tbody:last-child tr:last-child td{border-bottom:.1rem solid #004966}.text figure table thead:first-child tr:first-child th:first-child,.text figure table thead:first-child tr:first-child td:first-child{border-top-left-radius:.2rem}.text figure table thead:first-child tr:first-child,.text figure table thead:first-child tr:first-child th:last-child,.text figure table thead:first-child tr:first-child td:last-child{border-top-right-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:first-child{border-bottom-left-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:last-child{border-bottom-right-radius:.2rem}.text figure ul{text-align:left}.text figure ul.tree{padding-top:1.5em;padding-bottom:1.5em;border-radius:.2rem;background:#fff}.text figure ul.tree li{list-style-type:none}.text figure ul.tree .tree__item--folder::before{content:"ðŸ“";margin-right:.2em}.text figure ul.tree .tree__item--file::before{content:"ðŸ“";margin-right:.2em}.text figure figcaption{font-style:italic}.text pre{margin-top:1.5rem;font-size:inherit;line-height:1.5rem;padding:1.5rem 1.5rem .75rem;white-space:pre;overflow-x:scroll;overflow-y:hidden;scrollbar-color:#cccccc transparent;scrollbar-width:.75rem}.text pre::-webkit-scrollbar{margin-top:-.75rem;height:.75rem}.text pre::-webkit-scrollbar-track{background:0 0}.text pre::-webkit-scrollbar-thumb{height:.1rem;border-radius:.2rem;background:#ccc}.text hr{border:0;height:.2rem;background:#004966;margin-top:1.3rem}.text .admonition{position:relative}.text .admonition>*{padding-left:2rem}.text .admonition>:first-child::before{display:block;text-align:left;position:absolute;top:0;left:0;width:.75rem;font-size:1rem;line-height:1.5rem;height:1.5rem}.text .admonition--tip>:first-child::before{content:"â˜›";color:#fefefe;font-size:1.5rem;line-height:1.5rem;height:1.5rem}.text .admonition--warning>:first-child::before{content:"âš ï¸";color:#fefefe}.text .admonition--info>:first-child::before{content:"â“˜";color:#004966;font-weight:700}@media print{.text h1{margin-left:auto;margin-right:auto;margin-top:9rem;width:80%;font-variant:small-caps;text-align:center}.text--simple h1{margin-top:0}.text .meta{page-break-after:always;text-align:center;font-size:1.2rem;margin-top:1.5rem}.text .meta a{text-decoration:none}.text .meta time{display:none}.text .body p:first-of-type{margin-top:0}.text pre{padding:0;font-size:inherit;line-height:1.5rem;overflow:hidden;white-space:pre-wrap;word-break:break-all}}.text .podcasts{margin-top:1.5rem;text-align:center}.text a.podcast{display:block;height:3rem}.text a.podcast img{width:50%;max-width:200px}@media screen and (min-width:768px){.text .podcasts{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));grid-gap:1.5rem}.text a.podcast img{width:100%}}.text .tombstone{opacity:0}html,body{min-height:100vh}body{text-align:center}@media screen{body{color:#333;background-color:#fefefe}}@media print{body{color:#000;background:#fff}}@media screen{body{display:flex;flex-direction:column}main.page{width:100%;flex:1}}header.page,main.page,footer.page{text-align:left}.page-main,.contentblock{max-width:1200px;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto}@media print{.page-main,.contentblock{padding:0}}@page{size:A4;padding:0;margin:3rem}.highlight{color:#004966}.nobr{white-space:nowrap}.aria-only{display:none}@media aural{.aria-only{display:block}}.edit-on-github{font-size:.8rem;text-align:right;padding-top:1.5rem}.edit-on-github .container{max-width:1200px;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto}@media print{.edit-on-github .container{padding:0}}.edit-on-github a{background-color:#fe7f2d;color:#fefefe;text-decoration:none;display:inline-block;border-top-left-radius:.5rem;border-top-right-radius:.5rem;padding-left:.5rem;padding-right:.5rem;transition:.2s background-color ease-in-out}.edit-on-github a:hover{color:#fefefe;background-color:#004966}a.socialblock{display:block;border:0;line-height:1.5rem;position:relative;text-align:center}.hero a.socialblock img{float:none;display:block;margin-left:auto;margin-right:auto;height:3rem;width:3rem}.sharing{margin-top:1em}.sharing .resp-sharing-button__link,.sharing .resp-sharing-button__icon{display:inline-block}.sharing .resp-sharing-button__link{text-decoration:none;color:#fff;margin:.5em}.sharing .resp-sharing-button{border-radius:5px;transition:25ms ease-out;padding-right:.75em;padding-top:.1em}.sharing .resp-sharing-button span{font-size:.75em}@media screen and (max-width:798px){.sharing .resp-sharing-button span{display:none}}.sharing .resp-sharing-button__icon svg{width:1em;height:1em;margin:.4em;margin-top:.3em;vertical-align:top}.sharing .resp-sharing-button--small svg{margin:0;vertical-align:middle}.sharing .resp-sharing-button__icon{stroke:#fff;fill:none}.sharing .resp-sharing-button__icon--solid,.sharing .resp-sharing-button__icon--solidcircle{fill:#fff;stroke:none}.sharing .resp-sharing-button--twitter{background-color:#55acee}.sharing .resp-sharing-button--twitter:hover{background-color:#2795e9}.sharing .resp-sharing-button--pinterest{background-color:#bd081c}.sharing .resp-sharing-button--pinterest:hover{background-color:#8c0615}.sharing .resp-sharing-button--facebook{background-color:#3b5998}.sharing .resp-sharing-button--facebook:hover{background-color:#2d4373}.sharing .resp-sharing-button--tumblr{background-color:#35465c}.sharing .resp-sharing-button--tumblr:hover{background-color:#222d3c}.sharing .resp-sharing-button--reddit{background-color:#5f99cf}.sharing .resp-sharing-button--reddit:hover{background-color:#3a80c1}.sharing .resp-sharing-button--google{background-color:#dd4b39}.sharing .resp-sharing-button--google:hover{background-color:#c23321}.sharing .resp-sharing-button--linkedin{background-color:#0077b5}.sharing .resp-sharing-button--linkedin:hover{background-color:#046293}.sharing .resp-sharing-button--email{background-color:#777}.sharing .resp-sharing-button--email:hover{background-color:#5e5e5e}.sharing .resp-sharing-button--xing{background-color:#1a7576}.sharing .resp-sharing-button--xing:hover{background-color:#114c4c}.sharing .resp-sharing-button--whatsapp{background-color:#25d366}.sharing .resp-sharing-button--whatsapp:hover{background-color:#1da851}.sharing .resp-sharing-button--hackernews{background-color:#f60}.sharing .resp-sharing-button--hackernews:hover,.sharing .resp-sharing-button--hackernews:focus{background-color:#fb6200}.sharing .resp-sharing-button--vk{background-color:#507299}.sharing .resp-sharing-button--vk:hover{background-color:#43648c}.sharing .resp-sharing-button--facebook{background-color:#3b5998;border-color:#3b5998}.sharing .resp-sharing-button--facebook:hover,.sharing .resp-sharing-button--facebook:active{background-color:#2d4373;border-color:#2d4373}.sharing .resp-sharing-button--twitter{background-color:#55acee;border-color:#55acee}.sharing .resp-sharing-button--twitter:hover,.sharing .resp-sharing-button--twitter:active{background-color:#2795e9;border-color:#2795e9}.sharing .resp-sharing-button--linkedin{background-color:#0077b5;border-color:#0077b5}.sharing .resp-sharing-button--linkedin:hover,.sharing .resp-sharing-button--linkedin:active{background-color:#046293;border-color:#046293}.sharing .resp-sharing-button--telegram{background-color:#54a9eb}.sharing .resp-sharing-button--telegram:hover{background-color:#4b97d1}html,body{font-family:Arial,sans-serif;scroll-behavior:smooth;text-rendering:optimizeSpeed;-webkit-font-smoothing:antialiased;line-height:1.5rem;font-feature-settings:"liga" 1;font-weight:400;font-size:18px}@media(min-width:900px){html,body{font-size:20px}}@media(min-width:1100px){html,body{font-size:22px}}@media(min-width:1800px){html,body{font-size:24px}}@media print{html,body{font-size:18px}}hr{border:0;height:2px;background-color:#fe7f2d;margin-top:1.5rem;margin-bottom:1.5rem}.text{position:relative}.text .meta{font-variant:small-caps}.text .meta .byline{display:none}.text .meta .line{display:inline-block}.text .meta .calendar{display:none}.text h1,.text h2,.text h3,.text h4,.text h5,.text h6{font-family:Georgia,palatino linotype,book antiqua,Palatino,times new roman,serif;font-weight:400;page-break-after:avoid}.text h1 a,.text h1 a:visited,.text h2 a,.text h2 a:visited,.text h3 a,.text h3 a:visited,.text h4 a,.text h4 a:visited,.text h5 a,.text h5 a:visited,.text h6 a,.text h6 a:visited{color:#333;text-decoration:none}.text p,.text ul,.text ol,.text figure,.text dl,.text hr{margin-top:.75rem;font-size:1rem;line-height:1.75rem}.text q{font-style:italic}.text ul ul{margin-top:0}.text p>code,.text ul>code,.text ol>code,.text dt>code,.text dd>code,.text li>code{color:#004966}.text audio{display:block;width:100%;height:1.5rem;outline:none}.text audio span{line-height:1.5rem}.text dl{overflow:auto;position:relative}.text dt{font-weight:700}.text dd{padding-left:2rem}.text dd+dt{margin-top:1.5rem}.text ul{list-style-type:disc;padding-left:1.5rem}.text ol{list-style-type:decimal;padding-left:1.5rem}.text p{overflow-wrap:break-word;word-wrap:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}.text h1{font-size:2rem;font-weight:400;line-height:2.5rem;margin-top:1.5rem;margin-bottom:0}@media screen and (min-width:760px){.text h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem}}@media screen and (min-width:900px){.text h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1200px){.text h1{font-size:3rem;line-height:4rem;margin-top:3rem}}.text h2{font-size:1.5rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h3{font-size:1.25rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h4{font-size:1rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem;font-size:1.1rem}.text h5{font-size:.8rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text h6{font-size:.6rem;line-height:2rem;margin-top:2rem;margin-bottom:1.5rem}.text strong{font-weight:700}.text em{font-style:italic}.text code{font-family:lucida console,Monaco,monospace;speak:literal-punctuation}.text abbr,.text acronym{speak:spell-out}.text del{text-decoration:line-through}.text ins{text-decoration:none;font-style:italic}.text figure{position:relative;text-align:center}.text figure img{background:#fff;border-radius:.2rem;max-width:100%}.text figure table{min-width:75%;border-radius:.2rem;margin-left:auto;margin-right:auto;border:#004966;border-collapse:separate;border-spacing:0;background:#fff;margin-bottom:-.1em}.text figure table th,.text figure table td{padding-left:.75rem;padding-right:.75rem;line-height:1.3rem;border-left:.1rem solid #004966;border-top:.1rem solid #004966}.text figure table thead{background-color:#fff}.text figure table tr td:last-child,.text figure table tr th:last-child{border-right:.1rem solid #004966}.text figure table tbody:last-child tr:last-child td{border-bottom:.1rem solid #004966}.text figure table thead:first-child tr:first-child th:first-child,.text figure table thead:first-child tr:first-child td:first-child{border-top-left-radius:.2rem}.text figure table thead:first-child tr:first-child,.text figure table thead:first-child tr:first-child th:last-child,.text figure table thead:first-child tr:first-child td:last-child{border-top-right-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:first-child{border-bottom-left-radius:.2rem}.text figure table tbody:last-child tr:last-child,.text figure table tbody:last-child tr:last-child td:last-child{border-bottom-right-radius:.2rem}.text figure ul{text-align:left}.text figure ul.tree{padding-top:1.5em;padding-bottom:1.5em;border-radius:.2rem;background:#fff}.text figure ul.tree li{list-style-type:none}.text figure ul.tree .tree__item--folder::before{content:"ðŸ“";margin-right:.2em}.text figure ul.tree .tree__item--file::before{content:"ðŸ“";margin-right:.2em}.text figure figcaption{font-style:italic}.text pre{margin-top:1.5rem;font-size:inherit;line-height:1.5rem;padding:1.5rem 1.5rem .75rem;white-space:pre;overflow-x:scroll;overflow-y:hidden;scrollbar-color:#cccccc transparent;scrollbar-width:.75rem}.text pre::-webkit-scrollbar{margin-top:-.75rem;height:.75rem}.text pre::-webkit-scrollbar-track{background:0 0}.text pre::-webkit-scrollbar-thumb{height:.1rem;border-radius:.2rem;background:#ccc}.text hr{border:0;height:.2rem;background:#004966;margin-top:1.3rem}.text .admonition{position:relative}.text .admonition>*{padding-left:2rem}.text .admonition>:first-child::before{display:block;text-align:left;position:absolute;top:0;left:0;width:.75rem;font-size:1rem;line-height:1.5rem;height:1.5rem}.text .admonition--tip>:first-child::before{content:"â˜›";color:#fefefe;font-size:1.5rem;line-height:1.5rem;height:1.5rem}.text .admonition--warning>:first-child::before{content:"âš ï¸";color:#fefefe}.text .admonition--info>:first-child::before{content:"â“˜";color:#004966;font-weight:700}@media print{.text h1{margin-left:auto;margin-right:auto;margin-top:9rem;width:80%;font-variant:small-caps;text-align:center}.text--simple h1{margin-top:0}.text .meta{page-break-after:always;text-align:center;font-size:1.2rem;margin-top:1.5rem}.text .meta a{text-decoration:none}.text .meta time{display:none}.text .body p:first-of-type{margin-top:0}.text pre{padding:0;font-size:inherit;line-height:1.5rem;overflow:hidden;white-space:pre-wrap;word-break:break-all}}.text .podcasts{margin-top:1.5rem;text-align:center}.text a.podcast{display:block;height:3rem}.text a.podcast img{width:50%;max-width:200px}@media screen and (min-width:768px){.text .podcasts{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));grid-gap:1.5rem}.text a.podcast img{width:100%}}.text .tombstone{opacity:0}.event h1{font-size:2rem;line-height:2.5rem;margin-top:1.5rem;margin-bottom:1rem;font-weight:400}@media screen and (min-width:760px){.event h1{font-size:2.5rem;line-height:3rem;margin-top:1.5rem}}@media screen and (min-width:900px){.event h1{font-size:2.5rem;line-height:3rem;margin-top:3rem}}@media screen and (min-width:1400px){.event h1{font-size:3rem;line-height:3.5rem;margin-top:3rem}}.bios__bio{overflow-y:visible;margin-top:1.5rem;border-top:2px solid #fefefe;padding-top:1.5rem;margin-bottom:1.5rem;position:relative;display:flex;flex-direction:row-reverse}.bios__bio h2{line-height:3rem;margin-bottom:1.5rem;margin-top:0}.bios__bio img{margin-left:1.5rem;border-radius:50%;height:10.5rem;width:10.5rem}.bios__content{flex:1}@media screen and (max-width:768px){.bios__bio{display:block;position:relative;text-align:center}.bios__content{flex:none;text-align:left}.bios h2{text-align:center}.bios h2 span{display:block}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=UA-343802-37"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-343802-37')</script><meta name=color-scheme content="dark light"><link rel=stylesheet type=text/css href=https://cdn-prod.securiti.ai/consent/cookie-consent.css></head><body><script>(function(){var a=document.createElement('script'),b;a.src='https://cdn-prod.securiti.ai/consent/cookie-consent-sdk.js',a.setAttribute('data-tenant-uuid','2ea5dc1c-c928-410b-a379-78d825d2eb0c'),a.setAttribute('data-domain-uuid','2f40096a-75c8-4af2-aec3-42db0231a457'),a.setAttribute('data-backend-url','https://app.securiti.ai'),a.defer=!0,b=document.head||document.body,b.appendChild(a),a.addEventListener('load',function(){window.initCmp()})})()</script><header><div class=page-header><div class=logo><a href=/><img src="data:image/svg+xml,%3Csvg baseProfile=%22tiny%22 id=%22Capa_1%22 xmlns=%22http://www.w3.org/2000/svg%22 xmlns:xlink=%22http://www.w3.org/1999/xlink%22 viewBox=%220 0 650 210%22%3E%3Cg%3E%3Cg%3E%3Ccircle fill=%22%23fff%22 cx=%22100.91%22 cy=%22104.98%22 r=%2268.91%22/%3E%3Cg%3E%3Cpath fill=%22%23bf181a%22 d=%22M100.91 36.12C62.85 36.12 32 66.97 32 105.03c0 23.47 11.74 44.18 29.66 56.63v-56.63c0-21.68 17.57-39.25 39.25-39.25s39.25 17.57 39.25 39.25-17.57 39.25-39.25 39.25c-8.76.0-16.82-2.91-23.35-7.76v33.35c7.29 2.63 15.15 4.07 23.35 4.07 38.06.0 68.91-30.85 68.91-68.91s-30.86-68.91-68.91-68.91z%22/%3E%3Ccircle fill=%22%23efc700%22 cx=%22100.91%22 cy=%22105.03%22 r=%2222.96%22/%3E%3Cpath fill=%22%23e6b600%22 d=%22M100.91 84.32c12.3.0 22.31 9.68 22.9 21.83.02-.38.06-.75.06-1.13.0-12.68-10.28-22.96-22.96-22.96s-22.96 10.28-22.96 22.96c0 .38.04.75.06 1.13C78.6 94 88.61 84.32 100.91 84.32z%22/%3E%3C/g%3E%3C/g%3E%3Cpath fill=%22%233c3c3c%22 d=%22M213.3 128.11V81.98h19.29c10.51.0 17.3 6.16 17.3 15.7.0 9.39-7.11 15.95-17.3 15.95h-9.02v14.47H213.3zm18.71-23.13c4.84.0 7.61-2.66 7.61-7.29.0-4.54-2.7-7.04-7.61-7.04h-8.45v14.33H232.01z%22/%3E%3Cpolygon fill=%22%233c3c3c%22 points=%22272.12,128.11 272.12,81.98 304.8,81.98 304.8,90.71 282.39,90.71 282.39,100.36 302.88,100.36 302.88,109.15 282.39,109.15 282.39,119.38 304.87,119.38 304.87,128.11%22/%3E%3Cpath fill=%22%233c3c3c%22 d=%22M354.71 128.11l-8.2-14.35h-8.39v14.35h-10.26V81.98h20.89c10.24.0 16.85 6.24 16.85 15.89.0 6.29-3.19 11.52-8.56 14.12l9.56 16.11H354.71zM347.66 105.23c4.81.0 7.68-2.77 7.68-7.42.0-4.59-2.8-7.23-7.68-7.23h-9.53v14.65H347.66z%22/%3E%3Cpath fill=%22%233c3c3c%22 d=%22M470.47 129.01c-12.53.0-20.63-8.12-20.63-20.69v-6.53c0-12.57 8.1-20.69 20.63-20.69 12.72.0 20.63 7.93 20.63 20.69v6.53C491.1 121.08 483.19 129.01 470.47 129.01zm0-39.2c-6.59.0-10.37 4.2-10.37 11.52v7.43c0 7.32 3.78 11.52 10.37 11.52 6.59.0 10.36-4.2 10.36-11.52v-7.43C480.83 94.01 477.06 89.81 470.47 89.81z%22/%3E%3Cpolygon fill=%22%233c3c3c%22 points=%22543.01,128.11 524.48,99.79 524.48,128.11 514.41,128.11 514.41,81.98 523.93,81.98 542.46,110.3 542.46,81.98 552.53,81.98 552.53,128.11%22/%3E%3Cpath fill=%22%233c3c3c%22 d=%22M607.7 128.11l-3.01-9.41h-16.47l-3.01 9.41h-10.29l16.54-46.13h10.01L618 128.11H607.7zM601.98 110.29l-5.52-17.45-5.58 17.45h11.1z%22/%3E%3Cg%3E%3Cpath fill=%22%233c3c3c%22 d=%22M418.62 111.85c-.77 5.33-4.37 8.5-9.73 8.5-8.74.0-10.05-7.3-10.05-11.65v-7.3c0-4.35 1.31-11.65 10.05-11.65 5.35.0 8.95 3.15 9.73 8.45l9.52-1.42c-1.28-9.33-8.95-15.7-19.24-15.7-12.68.0-20.24 7.76-20.24 20.76v6.4c0 13 7.57 20.76 20.24 20.76 10.3.0 17.98-6.39 19.25-15.74L418.62 111.85z%22/%3E%3C/g%3E%3C/g%3E%3C/svg%3E" alt=Percona></a></div><input type=checkbox id=show-mobile-menu value=1 aria-hidden=true><div class=mobile-toggle aria-controls=mobile-menu aria-label="Open navigation" role=button><label for=show-mobile-menu id=hamburger><span class=hamburger-bar></span><span class=hamburger-bar></span><span class=hamburger-bar></span></label></div><div class=mobile-menu id=mobile-menu aria-labelledby=navigation-header role=dialog aria-modal=false aria-hidden=true><nav><ul class=menu__list><li><a href=/mongodb><span>MongoDB</span></a></li><li><a href=/mysql><span>MySQL</span></a></li><li><a href=/postgresql><span>PostgreSQL</span></a></li><li><a href=/projects><span>Projects</span></a><ul class="submenu__list sub-menu"><li><a href=/projects/operators/><span>Kubernetes Operators</span></a></li><li><a href=/projects/pmm/><span>Percona Monitoring and Management</span></a></li><li><a href=/projects/toolkit/><span>Percona Toolkit</span></a></li><li><a href=/projects><span>All projects</span></a></li></ul></li><li><a href=/podcasts><span>Podcasts</span></a><ul class="submenu__list sub-menu"><li><a href=/podcasts/><span>HOSS Talks</span></a></li><li><a href=/events/dave-stokes-talks-open-percona-live-2022/><span>Dave Stokes</span></a></li></ul></li><li><a href=/events><span>Events</span></a><ul class="submenu__list sub-menu"><li><a href=/events><span>Community Events</span></a></li><li><a href=/contribute/engineeringmeetings><span>Engineering Meetings</span></a></li><li><a href=/speakers><span>Speakers</span></a></li></ul></li><li><a href=/blog><span>Blog</span></a><ul class="submenu__list sub-menu"><li><a href=/blog><span>Posts</span></a></li><li><a href=/authors><span>Authors</span></a></li></ul></li><li><a href=/contribute><span>Contribute</span></a><ul class="submenu__list sub-menu"><li><a href=/contribute/articles><span>Articles</span></a></li><li><a href=/contribute/opentopics><span>Open Topics</span></a></li><li><a href=/contribute/youtube_playlists><span>YouTube Playlists</span></a></li><li><a href=/contribute/testimonials><span>Testimonials & Surveys</span></a></li><li><a href=/contribute/coc><span>Code of Conduct</span></a></li><li><a href=/contribute/documentation><span>Documentation</span></a></li><li><a href=/contribute/help-wanted><span>Help Wanted</span></a></li><li><a href=/contribute/videos><span>Videos</span></a></li><li><a href=/contribute/dev><span>Repositories and Artifacts</span></a></li></ul></li></ul></nav></div></div></header><main class=page><div class=page-main><article class="event text" itemscope itemtype=https://schema.org/BlogPosting><meta itemprop=image content="https://percona.community/events/streams-pg/pg-stream-week-2-march11-upd.jpg"><meta itemprop=mainEntityOfPage content="https://percona.community/events/streams-pg/2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm/"><meta itemprop=keywords content="Postgres, Stream"><h1 itemprop="name headline">Observing & Tuning Your PostgreSQL Workload Using PMM - Community PostgreSQL Live Stream & Chat - March 11th</h1><div class=meta><time datetime="2022-03-11 00:00:00 +0000 UTC" itemprop=datePublished><span class=line>March 11, 2022</span>
<span class=calendar><span class=day>11</span>
<span class=month-year>Mar 2022</span></span></time>
<span class=authors>by <a href=/speakers/charly_batista/>Charly Batista</a>, <a href=/speakers/matt_yonkovit/>Matt Yonkovit</a></span></div><div class=sharing><a class=resp-sharing-button__link href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on Facebook"><div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg></div><span>Share on Facebook</span></div></a><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Observing%20%26%20Tuning%20Your%20PostgreSQL%20Workload%20Using%20PMM%20-%20Community%20PostgreSQL%20Live%20Stream%20%26%20Chat%20-%20March%2011th&url=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on Twitter"><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div><span>Share on Twitter</span></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f&title=Observing%20%26%20Tuning%20Your%20PostgreSQL%20Workload%20Using%20PMM%20-%20Community%20PostgreSQL%20Live%20Stream%20%26%20Chat%20-%20March%2011th&source=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on LinkedIn"><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6.0 2.5 1 2.6 2.5.0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/></svg></div><span>Share on LinkedIn</span></div></a><a class=resp-sharing-button__link href="https://telegram.me/share/url?text=Observing%20%26%20Tuning%20Your%20PostgreSQL%20Workload%20Using%20PMM%20-%20Community%20PostgreSQL%20Live%20Stream%20%26%20Chat%20-%20March%2011th&url=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on Telegram"><div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M.707 8.475C.275 8.64.0 9.508.0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102.0 001.75.527l2.96-2.41a.405.405.0 01.494-.013l5.34 3.87a1.1 1.1.0 001.046.135 1.1 1.1.0 00.682-.803l3.91-18.795A1.102 1.102.0 0022.5.075L.706 8.475z"/></svg></div><span>Share on Telegram</span></div></a></div><div itemprop=articleBody class=body><p>Organize and access easily to your database data by observing and tuning your PostgreSQL workload using PMM. Our experts Matt Yonkovit and Charly Batista present use cases in a bi-weekly live stream about Postgres-related topics.</p><h2 id=video-part-01>Video Part 01</h2><div class=youtube__block><a href=https://youtu.be/H2uNDbQ8lCk class=youtube__img target=_blank><img width=480px src=https://img.youtube.com/vi/H2uNDbQ8lCk/hqdefault.jpg alt></a><p>Link: <a href=https://youtu.be/H2uNDbQ8lCk class=youtube__link target=_blank>https://youtu.be/H2uNDbQ8lCk</a></p></div><h2 id=video-part-02>Video Part 02</h2><div class=youtube__block><a href=https://youtu.be/SxQyC2RdBCg class=youtube__img target=_blank><img width=480px src=https://img.youtube.com/vi/SxQyC2RdBCg/hqdefault.jpg alt></a><p>Link: <a href=https://youtu.be/SxQyC2RdBCg class=youtube__link target=_blank>https://youtu.be/SxQyC2RdBCg</a></p></div><h2 id=transcript-part-01>Transcript Part 01</h2><p><strong>Matt Yonkovit:</strong><br>Here we go! We&rsquo;re live! Hello everyone! Welcome to a specially exciting Friday live stream. Hopefully everyone is getting to the weekend. I am here today with our esteemed Charly, Charly, our tech lead for Postgres. How&rsquo;re you doing this week?</p><p><strong>Charly Batista:</strong><br>Hey, man, I&rsquo;m doing great. I&rsquo;m doing great. Thanks. Yeah.</p><p><strong>Matt Yonkovit:</strong><br>So Charly&rsquo;s going to be joining me every two weeks. Okay, so we&rsquo;re actually going to be going live every two weeks. With a specific Postgres topic. We&rsquo;ve actually got 20 weeks mapped out. So 20 weeks of Postgres awesomeness. And we&rsquo;re going to be building upon all of the Postgres stuff that we&rsquo;ve been doing in previous streams. So if you follow along, you come along for the journey, you&rsquo;re going to find some really interesting things, there&rsquo;s going to be lots of different topics, we hope that you will swing by every Friday, every other Friday, every other Friday at 2pm. Eastern. And we&rsquo;re going to also be doing this with MySQL starting April 1, with Marcos Albe, who&rsquo;s going to be joining us to go through the exact same things, but for MySQL, so if you are interested in one or the other, so every week, there will be something streaming from Percona. And we&rsquo;ll be able to cover whatever interesting topics of the week come up. And you&rsquo;ll be able to end the week on a good note with me, Charly or Marcos. So that should be exciting, right? I&rsquo;m excited. I am. I&rsquo;m personally excited about you.</p><p><strong>Charly Batista:</strong><br>Yeah! that&rsquo;s totally, that&rsquo;s looks really, really awesome. And the good thing is, even though we have sort of an agenda, and so this is an open talk, right? So we can talk about what the audience what the people there and they want to discuss, like, for example, we are being asked if we&rsquo;re going to talk about PostGIS, right, well, maybe not today, because we haven&rsquo;t prepared but like, those are things that we were really interested to hear from what they want to talk to discuss.</p><p><strong>Matt Yonkovit:</strong><br>Absolutely, yeah. And so we want to make this open for folks. And right now, we generally get more people who watch after the stream than we do during but that&rsquo;s okay. So if you do have questions after the stream, you can ping us offline, you can drop me a line. You can see my socials listed on the screen. Actually, I&rsquo;m going to add yours, Charly, in future streams, because since you&rsquo;re going to be here for 20 weeks. Yeah, we should list where people can reach you as well.</p><p><strong>Charly Batista:</strong><br>Definitely, I&rsquo;m kinda unsocial on this perspective. So, but yeah, I know, I do have. I have the Twitter and Facebook things. But like, I really don&rsquo;t news that often I need to learn like, I&rsquo;m an old person. I&rsquo;m an older guy.</p><p><strong>Matt Yonkovit:</strong><br>I&rsquo;m older than you. So you can&rsquo;t claim to be the old man on the street.</p><p><strong>Charly Batista:</strong><br>It&rsquo;s the perspective right? So, there are aged people, and there are old people. Right?</p><p><strong>Matt Yonkovit:</strong><br>I am aged. Is that what you&rsquo;re saying?</p><p><strong>Charly Batista:</strong><br>I didn&rsquo;t mean to say that. You&rsquo;re no but like, I better be aged. The old it&rsquo;s I think it&rsquo;s like just like whiskey or wine. You&rsquo;re not old, you&rsquo;re aged, so you get better and better and better.</p><p><strong>Matt Yonkovit:</strong><br>Just take yourself deeper.</p><p><strong>Charly Batista:</strong><br>Look, it looks how it looks like I&rsquo;m actually myself now who the more they strangle bid, the more hook the gap. So I think I got to stop. Yeah,</p><p><strong>Matt Yonkovit:</strong><br>Yeah, that&rsquo;s probably a good thing. Right. So to slow down a little bit on that. But we are not here just to talk about the weekend and talk about Charly telling me that I&rsquo;m old. We are here to talk about tuning your Postgres instances and figuring out what sort of configuration variables you could, you should change based on what sort of workload is running. And I think that that is an exciting topic that many people have questions on. And we&rsquo;re hoping that you out there and userland are watching and you are eager to see how to do some tuning.</p><p><strong>Charly Batista:</strong><br>Yep. Yeah, definitely. Let&rsquo;s start here, right, shall we?</p><p><strong>Matt Yonkovit:</strong><br>He just jumped right into, he just, yeah. This. So we have three workloads today that we&rsquo;re going to look at. So Charly created a system that&rsquo;s that has a decidedly Charly workload, which means that it&rsquo;s a box that&rsquo;s falling over and going to kind of fall over dead because it&rsquo;s old and decrepit. aged.</p><p><strong>Charly Batista:</strong><br>Thatâ€™s totally sad. That&rsquo;s yeah, that&rsquo;s a bad thing.</p><p><strong>Matt Yonkovit:</strong><br>And I&rsquo;ve got two separate workloads that I&rsquo;m going to throw at Charly as well.</p><p><strong>Charly Batista:</strong><br>Yeah. All right. So yeah, before I go, if your work a little bit, I have no idea haven&rsquo;t seen. That&rsquo;s, that&rsquo;s a good thing. So because of, it&rsquo;s usually how we do at Percona, right? So we have customers, they, they have a problem in their workload, open a ticket, and then sometimes we need just to jump in to help them without no background. So that&rsquo;s your workload. Well, my workload, even though I have some background here, and then not much in on my workload, only thing I&rsquo;m doing here, it&rsquo;s a load test, I&rsquo;m using pgbench. Really, really, really simple stuff, I want to show you the common in the field. But there is nothing special here. So as we can see, from this result, and get around 17 1800, sometimes 2000 transactions per second, right? So not impressive. And the customer, the user wants to know why our database is behaving slowly, slowly. This is not a super harder, I should say. But if we take a look, let&rsquo;s take a look on the specs of my heart are here. We have eight CPUs, we have 32 gigabytes, RAM memory, and we have 512 gigabytes disk. So it&rsquo;s a decent box, let&rsquo;s say that could do a lot better than 1500 transactions per second, right? So and then my customer might is, is asking why it&rsquo;s so slow, it&rsquo;s behaving so slow. So we need to go here and start investigating what problems we might have. So we have many, many different approaches. The one that we are going to use here is not the only one that we can use. And sometimes you can also find a different approach that works better for you. So this is fine, right? But one thing that we need to keep in mind is, whatever approach we use, we need to be methodic. Right, we need to use a method, and go to that method. So and this is what we&rsquo;re trying to go here. So but when I do start to investigate, usually there are three things are sometimes four they go first. So first things is CPU usage. So I usually take a look on my CPU on the CPU to see how it&rsquo;s being used. Well, the CPU is under 75% utilization here. There are some extra information I&rsquo;m not going to look at now. But like just CPU wise, not a bad. It&rsquo;s a 75% utilization here, right? So if I go for my disk utilization, or let&rsquo;s, let&rsquo;s see memory, because I read here, so memories are bad. So we see, we here use a memory, like we use it memories, like just this small portion in yellow here, we still have a lot of free memory that we can use. If you take a look on the swap, we almost didn&rsquo;t use any swap, we see some sub activities, here and there, which is usually not good. We don&rsquo;t want to see those activities. But like, it doesn&rsquo;t seem to be the problem. Especially because, well, the server is behaving badly for a long time. Not only those those those period of times here. It&rsquo;s getting some IO but it&rsquo;s a database, right? It should read and write so it&rsquo;s writing, like reading more than writing on my database here. So and the disk load here, it&rsquo;s I we have some lazy load here but like the right load, it shouldn&rsquo;t be that bad. So what would you say, Matt? From what we see here? What do you do have a hint for what is the problem what where we should go just for that basic information here. Let me just say</p><p><strong>Matt Yonkovit:</strong><br>A lot of memory you probably have things Yeah, and you are seeing some disk IO there.</p><p><strong>Charly Batista:</strong><br>Yeah, we do have some disk IO, you see here.</p><p><strong>Matt Yonkovit:</strong><br>No, we&rsquo;re probably looking at some memory settings. and so that&rsquo;s where you&rsquo;re probably not, don&rsquo;t have enough being put into memory reading too much from disk.</p><p><strong>Charly Batista:</strong><br>Okay, so if we go for memory settings on Postgres, we&rsquo;re talking about which do you do have on top of your mind? Which configuration? We go from memory setting? If you don&rsquo;t have it&rsquo;s fine. Like even, I sometimes?</p><p><strong>Matt Yonkovit:</strong><br>Yeah, like you&rsquo;re talking like shared buffers, you&rsquo;re talking? A couple of others.</p><p><strong>Charly Batista:</strong><br>Okay? Yeah, let&rsquo;s, let&rsquo;s take a look at the shared buffers,</p><p><strong>Matt Yonkovit:</strong><br>Charly asks me questions and tries to quiz me during live streams, just so everyone&rsquo;s aware. Like, like, everyone else is just like let me show you what to do. And I&rsquo;m off like getting in answering questions behind the scenes. And then Charly starts throwing me like questions. And I&rsquo;m like, what? I need to pay attention.</p><p><strong>Charly Batista:</strong><br>Yeah, please. we need to keep it interactive. otherwise, people going to sleep out there. We don&rsquo;t want people sleeping. Right? So at least someone should be under pressure</p><p><strong>Matt Yonkovit:</strong><br>Okay, so now I&rsquo;m gonna go invite other people so they can be under pressure, not me.</p><p><strong>Charly Batista:</strong><br>While you Oh, yeah, we are free to ask for help. Why not?</p><p><strong>Matt Yonkovit:</strong><br>So who wants to come help me out? Live stream while I&rsquo;m here trying to do this. But anyways, go on, Charly.</p><p><strong>Charly Batista:</strong><br>Oh, yeah. You said shared buffers, right. So let&rsquo;s, let&rsquo;s take a look on the shared buffers. We have here eight gigabytes for shared buffer, which is not bad, right, like recommendations for Postgres is not used to match. I have 32 gigabytes of RAM for the OS. So if we have eight gigabytes, shared buffers, let&rsquo;s say the OS uses two gigabytes, so we are 1514 gigabytes for the kernel cache, right? No as cache, that&rsquo;s supposedly, to help us with IO. Right? So I would say this is value is not bad, like, for this box should should be fine. What else could you would you take a look? Well, if you want you to look at anything here, so I just change it from one dashboard. This dashboard is OS perspective, and this is the postgres dashboard. So just to make sure we&rsquo;re on the same page, so this is a postgres dashboard.</p><p><strong>Matt Yonkovit:</strong><br>so one of the got a lot of connections, right? So each of the nations is going to consume quite a bit of memory.</p><p><strong>Charly Batista:</strong><br>Okay, yeah, that&rsquo;s a good point, we got a lot of connections. We didn&rsquo;t take a look of how many connections you have. Right. So and yeah, as you mentioned, here, we are over, I would say 500 connections. Right. That&rsquo;s, that&rsquo;s, that&rsquo;s quite a lot for this box. Right. So and it may say something. So we did find one problem, we have too many connections for this box.</p><p><strong>Matt Yonkovit:</strong><br>Right? Absolutely. So there should be some connection pooling or something there to limit those.</p><p><strong>Charly Batista:</strong><br>Indeed, indeed, and we&rsquo;re going to try with the connection pooling later. Before we do any change, we got to adjust switch from direct connections to the connection pool to see if we get any improvement, right. Just like without changing any configuration. Just going to stop my load and point to a connection pool because I have here a connection I have a PG bouncer there, but just minimize configuration, listening on the port 6532. So we&rsquo;re gonna jump and see if we change anything. But before we do that, let&rsquo;s get the investigation. Okay, yeah, we do have two main connections. For this instance. We have like over 500 connections here. And if we take a look here on the activity itself, it doesn&rsquo;t look to have too many things doing because something is really putting my server on its knees right. And let&rsquo;s go back to the last</p><p><strong>Matt Yonkovit:</strong><br>Yeah, yeah, it looks like you&rsquo;re doing quite a bit of checkpointing as well. So I&rsquo;m cheating and looking at graphs in the background. You&rsquo;ve got some pretty big spikes that are happening on a regular basis. Look at that. Big checkpoint stats but</p><p><strong>Charly Batista:</strong><br>it does, yeah, it does. And some from time to time off also we just flush a lot of buffers like it&rsquo;s expected because of checkpoints right. So now you can ask whatever information for us to take a look here it&rsquo;s fine. We are investigating right so sometimes I myself just miss someone point on order. So order a couple of eyes. They definitely kind of out. Okay, we have Two things, too many connections, and we have a lot of checkpoints. Checkpoints are bad. Or they are not bad proceed. But like when we there too often they&rsquo;re bad because they bring problem with IO, right? And okay, one thing that I want to show you here, remember that I said there are some hidden information on the CPU dashboards. You see this red stuff? Yeah, this is the I O wage. And this is pretty bad. So what is the IO weigh thing on this graph? What is it telling me here? Can you explain to us like in one short sentence,</p><p><strong>Matt Yonkovit:</strong><br>it&rsquo;s basically you are waiting for IO to complete before you can use the CPU.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s perfect. So the IO way is</p><p><strong>Matt Yonkovit:</strong><br>It was like it wasn&rsquo;t perfect, but you couldn&rsquo;t think of a good way to tell me it wasn&rsquo;t. And then you were kind of I&rsquo;m pretty sure that Charly was trying to thought.</p><p><strong>Charly Batista:</strong><br>I wouldn&rsquo;t have asked you that if I didn&rsquo;t have confidence on your own. And that&rsquo;s why I&rsquo;m asking you do have confidence. You know. And that&rsquo;s the theme. So we&rsquo;re the CPU is wasting a lot of time waiting for IO. That&rsquo;s the main reason why our CPU is underutilization here. I will say, if you take a look, this green stuff here is the time that the userspace Postgres is using CPU. So it&rsquo;s almost nothing might even the kernel, the system&rsquo;s system time here is doing almost nothing. So and this huge amount of disk utilized, not utilization, but waiting time is killing our server. But this is the major problem that we have here. We have a huge IO bound system in this case, yeah, right. And this is killing our database performance. Well, we need to find a way to deal with those things. There are parameters on both screens that can help us and also their actions that we can do from the kernel as perspective, well, at some point, we cannot scale, and we need to either split the load between two different servers, or get better hardware for disk. It might we might get to that point. But we don&rsquo;t know yet. So for what the only thing that we know, at this moment, is that we&rsquo;re putting too much pressure on our IO subsystem. Right? Okay. Before we change anything, remember that I told you, I would change to use. Just show you this is the command I&rsquo;m using here, right? I got to change just the port here. Instead, use the connection directly to the database. I got to use PG bouncer. So my PG bouncer has the configuration to only send a max of 100 connections to the database.</p><p><strong>Matt Yonkovit:</strong><br>So you&rsquo;ve already set up PG bouncer.</p><p><strong>Charly Batista:</strong><br>It&rsquo;s yeah, I read have one. Yeah, it&rsquo;s there. So I can show you like in a second I feel. And well, if we take a look, looks like we got some improvement. Doesn&rsquo;t seem to be all it depends.</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;s significant. It&rsquo;s more consistent. Yeah, it is. It is.</p><p><strong>Charly Batista:</strong><br>Exactly. So.</p><p><strong>Matt Yonkovit:</strong><br>But you can see it in PMM with the different graphs, right?</p><p><strong>Charly Batista:</strong><br>Mm-hmm. Okay, it&rsquo;s updating here. But even then, we still put a lot of pressure, right? It&rsquo;s still we still see here like from OS perspective, let&rsquo;s take a look on the connection. So the connection should drop. Yeah, it dropped here. See, this is when we stopped the load. And here is where we got the old back with PG bouncer. So we have less than half of the number is connected right. So the max number of connections sometimes we&rsquo;re going to have more here because the connection dies and it stays open for some period of time waiting for for for the kernel to release them. So, that&rsquo;s expected to see more on the max number of connections we put here, so that&rsquo;s fine.</p><p><strong>Matt Yonkovit:</strong><br>When you start to look like for instance, even like your tuple activity there, you could see a pretty significant increase. Because you&rsquo;re not fighting for resources, right?</p><p><strong>Charly Batista:</strong><br>That&rsquo;s true. So that&rsquo;s true. And if Yeah, that&rsquo;s that&rsquo;s a really good point. So even though we still have some limitations here, right, we still have the physical limitation, but now we are we don&rsquo;t have so many.</p><p><strong>Matt Yonkovit:</strong><br>He was gonna say so many connections, but it looks like we have an internet hiccup. So I will just continue on for him for a second. Hopefully, it is not me. So, while we try to get Charly back here. Basically, what we&rsquo;re seeing is the number of connections where can the connection pooler? Okay, you&rsquo;re back? You disappeared for a second. And now you&rsquo;re back.</p><p><strong>Charly Batista:</strong><br>Oh, so sorry.</p><p><strong>Matt Yonkovit:</strong><br>Oh, no, he&rsquo;s frozen again. Oh, the negatives of having internet on a Friday afternoon, right? Of trying to do the live stream. But no, so it is interesting because of the architecture for connections and threads within Postgres, there is a lot of connection and potential overhead. But tools like PG, bouncer, PG pool, are absolutely required to get the optimal performance on a lot of these. So we are seeing what&rsquo;s going on. Oh, and look Charly is back. Hello, Charly, welcome back to the live stream here.</p><p><strong>Charly Batista:</strong><br>I just got the glitch on my connection here.</p><p><strong>Matt Yonkovit:</strong><br>Oh, sorry. I was just saying a little bit about how Postgres is threading model and the connections make it. So if you do have a lot of users who are going to be accessing things concurrently, there, is it important to have either connection pooling or something out there to help manage those? Now, David Gonzalo asked, is there a recommended sizing relation between the maximum numbers of sessions in PG bouncer pool versus the CPUs in the database node.</p><p><strong>Charly Batista:</strong><br>The max number of connections on from we have two different things of max number of connections on PG bouncer, we have the max number of connections from the front end. That is the connection that PG bouncer receive is from the Postgres clients. And we have the max number of connections that PG bouncers opens on the database. Which one is he referring to?</p><p><strong>Matt Yonkovit:</strong><br>Not 100% Sure, because the question is what the question is, right? So just when you&rsquo;re talking about setting PG bouncer, is there some magic formula for saying if you&rsquo;ve got eight CPUs, you shouldn&rsquo;t have more than 80 connections set up in the connection in PG bouncer or something like that is there like some formula that we should be using?</p><p><strong>Charly Batista:</strong><br>The thing is PG bouncer is single threaded. It use a synchronous IO to balance. So what it does is when it receives a correction, it puts in the pool, and then waits for the kernel to notify when something is back, just like normal synchronous io we have like a pool, or are io running on Linux? Right? So it all use Oh, it&rsquo;s only has one trash. So, it doesn&rsquo;t matter if you have 100 CPUs and your PG bouncer. If you just install your PG bouncer in use as it is it only uses one CPU because it&rsquo;s single-threaded. There are some tricks we can do to make PG bouncer to use multiple CPUs. But those are some tricks we need to do by default, like my here, my PG bouncer here. I didn&rsquo;t do anything, just using one. So, but because it does use a synchronous IO, it can handle really 1000s of connections. So because it doesn&rsquo;t get busy all the time with with the processing from the IO</p><p><strong>Matt Yonkovit:</strong><br>But from the pool perspective is it something that you want to say like you&rsquo;ve limited here to 200 I think connections? Is there, is there a limit to the number of connections you should allow to Postgres based on the size of the box?</p><p><strong>Charly Batista:</strong><br>It is. Ah, so I did limit here to 200. Because my intention was to show the difference of what we get, if we have like, just around half of the number of connections from the resource perspective, ideally, we should not have more than usually like six trades per CPU concurrently. Right. So in my case here, I would say, ideally, I would put 64 Max connections as I have eight CPU cores. So I read put a lot of pressions here, because they&rsquo;re going to have an around eight process, so in average, but it&rsquo;s fine. So, we will then have two main context switching and so it&rsquo;s fine. But when we are moving for a number that&rsquo;s higher than that value, all the bowsprit is beside. So we can do the box start struggling with resources?</p><p><strong>Matt Yonkovit:</strong><br>Okay. Fair enough. We got a comment from Augustine, who said, our mics were a little hot or cold, depending on which of the US was talking. Hopefully, that clears it up a little bit. I made some adjustments if the sound went a little wonky. Oh, and Augustine says, much better, much better for both. So that&rsquo;s, that is good. So I&rsquo;m glad someone out there is, is watching and letting us know. Yeah. If we&rsquo;re going to do this every week, we want to save these settings. So we don&rsquo;t have to go through that. But okay, so Charly, where we were looking at the connections, we dropped the connections, we saw that we were getting a lot more throughput based on that.</p><p><strong>Charly Batista:</strong><br>So take us from still have a problem here.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, we are still seeing too much IO.</p><p><strong>Charly Batista:</strong><br>Exactly. And if we keep the load. At some point, it will just get to the point it was before, right? Because when we added the PG bouncer, we&rsquo;re saving CPU ressource from the Postgres side. So now Postgres has more CPU to play to work. But the IO is still the same, right? So didn&rsquo;t do much.</p><p><strong>Matt Yonkovit:</strong><br>Not expect the connection pool to make a big difference on the IO unless you were saying I&rsquo;m writing the IO with a lot of connections, right? So I mean, it&rsquo;s it is possible, if you reduce the number of connections that were running concurrently, you could also reduce the IO, just because you&rsquo;d have less things hitting disk, but that would probably reduce your overall throughput to achieve that.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s true, that&rsquo;s true. Okay, yeah, from here, so well, I am really curious to see who is using that much of IO. So there are some tools on Linux there, they really help. So remember, PMM gave us a really good direction, where we go, right. And there are some really nice tools on Linux that we can use to find the problem. One nice thing that I really like on PMM is that we can extend PMM to use those tools. This is not the top for today. So we probably should get one-off, we schedule another one just to go for through PMM to see how we can extend them all just kind of stuff, right? So I&rsquo;m not going to spend much well, because</p><p><strong>Matt Yonkovit:</strong><br>That means generally you just sign up for two new streams, right? So you sign up for one in PostGIS, and now you&rsquo;ve signed up one for extending PMM for additional Postgres metrics. So you heard it here, folks, we are now up to 22 weeks. So, if there&rsquo;s anybody who wants another topic or two, let&rsquo;s see if we can get to 25 weeks of Postgres content by the end of this stream.</p><p><strong>Charly Batista:</strong><br>The good thing is that for that PMM one I can outsource.</p><p><strong>Matt Yonkovit:</strong><br>no, you sign up, you signed up. It&rsquo;s you. It&rsquo;s you. I already got you on a motorcycle graphics that we&rsquo;re using.</p><p><strong>Charly Batista:</strong><br>Alright, that sounds interesting. So really good. Let&rsquo;s move back here before I say something more.</p><p><strong>Matt Yonkovit:</strong><br>Yes, those who are watching like, like I saw Augustine was watching you can thank me that I didn&rsquo;t let Charly outsource this to you. So just be mindful of that I just saved you or maybe a couple of other people who might have gotten looped in here.</p><p><strong>Charly Batista:</strong><br>Yeah, okay, I got to work my way later on. Okay, let&rsquo;s move on here. So I gotta check my IO top here, because I want to see who is destroying my disk. Well, on my IO top here, I always see this check pointer guys see, this guy here and this background, right? They&rsquo;re always here. And sometimes they write like 100 megabytes per second. Sometimes they write 320. They go zero. So those guys are always here, right? The check pointer in the background, right? Indeed. So well, we got another hinge. So we have problems about IO. And we have at least two guys here from Postgres that are really destroying my IO subsystem here. Okay, what can we do with those guys? So before we move on, we need to understand what those did this? Let&rsquo;s work for this checkpoint or here. Because you mentioned before, well, let&rsquo;s go back to the graph. We mentioned before, that from time to time, we do have some checkpoints activity, right. So we had a huge spike. Well, they&rsquo;re out of my graph.</p><p><strong>Matt Yonkovit:</strong><br>if you go back and look at the last hour, you&rsquo;ll start to see him show up. It&rsquo;s fine.</p><p><strong>Charly Batista:</strong><br>I can simulate them, or we can even force them to appear here. Let&rsquo;s try to force them right. So as we know what checkpoint is. Oops, I just stopped with my</p><p><strong>Matt Yonkovit:</strong><br>Yeah, well, that would definitely have an impact on those. Yeah, stopping.</p><p><strong>Charly Batista:</strong><br>Okay. Oh, I shouldn&rsquo;t stop. So. Let me go here to. Okay. Can you hear me, it looks like my connection. Just. I still hear you. Okay, can you also see me right, because</p><p><strong>Matt Yonkovit:</strong><br>I can see you, although your screen went really tiny. So?</p><p><strong>Charly Batista:</strong><br>Yeah, no, no, it also happened to me. I think this is the database server. It&rsquo;s not responding. That&rsquo;s why I thought my connection was lost.</p><p><strong>Matt Yonkovit:</strong><br>So Charly is streaming, but his database connections dropped. Okay. Yeah. Internet, Gremlins, everyone. Always on a Friday afternoon.</p><p><strong>Charly Batista:</strong><br>Okay. Okay, looks like I&rsquo;m back. So I want to be Postgres.</p><p><strong>Matt Yonkovit:</strong><br>By the way, where are these servers? Are these in AWS?</p><p><strong>Charly Batista:</strong><br>AWS? Yeah, these are in AWS. And this is another thing that we should talk later because when we start using too much IO, and if you&rsquo;re not paying for that I was using AWS, they just reduce our IO right. So sometimes it happens.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, I mean, and I think depending on what class a box there&rsquo;s also other things right? Because you can get bursts. Yeah, you burstable CPU works great until you run out of your burstable allotment, and then all of a sudden, it dogs is that there&rsquo;s there&rsquo;s all kinds of gotchas in little videos</p><p><strong>Charly Batista:</strong><br>Exactly. Yeah. So I got a fresh checkpoint here. So expect now my database we have like huge impact we should have. This is when I dropped the connections, but now the queries per second should really go down. Because see, from time to time, we have zero transactions per second. Because well, what I&rsquo;m doing, I&rsquo;m forcing the database checkpoint to flush all its cache to the disk, right?</p><p><strong>Matt Yonkovit:</strong><br>That&rsquo;s why you see the duration of like, look at the duration of transactions taking you to see a spike. So everything&rsquo;s having to wait.</p><p><strong>Charly Batista:</strong><br>Exactly. So don&rsquo;t do it in production unless you have a really good reason. Right. So what we want to do here is we want to show what is the impact of each full checkpoint on the database side. It&rsquo;s kind of a stop the world and wait for me to write everything that I need to write to the disk. Is it still ongoing? So it looks like we have a lot of data, to write to the disk. So how does this work on Postgres, Postgres needs to write dirty data to the disk, but every, every time not only post all the database, every time that we do a transaction, let&rsquo;s say we do an update. So when we send a commit, that update is first the data source updated in memory, right. And if it&rsquo;s been updated in memory, it needs to be flushed to the disk. Well, there are many approaches that different databases, they use different approach. One thing that we can do is we can go to the data tables, like the data files, and right now, that data, well, that&rsquo;s expensive, because usually, the data files there, the data is just distributed randomly, especially for Postgres, right. So it&rsquo;s not saved in this sequentially, like one after another, let&rsquo;s say create the table user, and I have the ID, that is my primary key inside the table user. That doesn&rsquo;t mean that in the disk, the data file will be organized by the ID, it&rsquo;s not. When we&rsquo;re inserting Postgres tries to put one after another. But when we start updating things, there&rsquo;s going to shift in here and there. So it&rsquo;s inefficient to write every time. Another problem is if we start writing to the data tables from memory, if something happens to the box, in the middle of the process, we&rsquo;ll be left with inconsistent data on the data files, right. So it&rsquo;s not only inefficient, it&rsquo;s dangerous as well. So because it&rsquo;s inefficient and dangerous, most of the databases, at least the ones that I know, they use a modern approach, they save this data somewhere else. They usually have a either a circle a file, or they have a file that they pre-created that is sequential. So they will say that the first in their file, some database, use undo log files or database like post misuse wal files that the right ahead logs. So if something happens, why I&rsquo;m writing the data to that files, and the data, let&rsquo;s say the database crashes, or the worst crashes or whatever something happens with the database. When we restart the database, the database can check back to the old files. If there is a transaction in the middle, it just rolls back the transaction. And the data files are still consistent later on the database transfer everything from the wal files to the data files, right. So those are when we&rsquo;re doing checkpoints. But you can tell but well, a crash can still happen during the process. So we still can have a crash now during the checkpoint time. Right? So then it didn&rsquo;t do anything for the safety. Well, if a crash happens, while the database is copying data for the wildfires, to the data files, when the database restarts, it will see that the data fires is inconsistent. But then it can find where was on the wildfires, because the wildfires are saved or kept safe. They&rsquo;ve been written already. And it can redo that transaction and recover the data files. So this is a trade-off. If we wait for the database to do not flush or to do not just forgot checkpoint to do not checkpoint too often and keep a lot of data.</p><p>When the database crashes, and it recovers, it might need to read a lot of files to get the data of the database consistent to where it crashed before. So the recovery time when do they if the database crashes, and we need to restart the database is going to be longer and fun if we have more checkpoints. So, for the other hand, if we have checkpoints too often, we can see a problem that we&rsquo;re seeing here so we can put the disk really under pressure. So it can really create the problem that we were seeing here, that we have a lot, a lot of this activity, then we, we start struggling with the number of transactions that we can put to the database. So we need to find a way to balance both settings. And this is the first thing that we should go. So there are some settings. The first one is we can increase the wal size. And by default, if I&rsquo;m not mistaken, there are two default there are two configurations for the wal size, there is the minimum wal size and the max wal size. The minimum wal size, it says if we flush your checkpoint, not flush, if to a checkpoint like I&rsquo;m doing now. So it will write everything right. If you clean up all the wal files, we&rsquo;re going to start fresh from here. So then it will start creating the new files to start filling those files. So these these, these smaller the minimum wal size. So when the database starts fresh from the wal files, what is the size of the wal file? This is how we define is defined by the mean wal size file.</p><p><strong>Matt Yonkovit:</strong><br>And then it grows from there. Because I think the default does 80 or 100. I think it&rsquo;s 80. Yeah,</p><p><strong>Charly Batista:</strong><br>Yeah, it&rsquo;s pretty small. I think it&rsquo;s 80. Yeah, I think it&rsquo;s around 100 megabytes. Yeah, I think around that I don&rsquo;t recall top of my mind, we can double-check. And the max is one gigabyte. So for workloads, like the one that we had, here, those are too small, right? So I want to have larger, so I can afford a longer time during the recovery process, but my client can afford that. So then he told me, I want to improve performance. And we can afford, like we have applications. So if decide this, this database crashes, and they can do some order high availability operations and get things back. Okay. So then the first thing that we can do, and we should do is start increasing the wal size, right are going to increase, both are going to increase the main wal size and the max. And then you can ask me, okay, what is the best value? Well, what is the best value?</p><p><strong>Matt Yonkovit:</strong><br>You said they ask you?</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s a good question. That&rsquo;s a good question. I had no idea. Because, well, I&rsquo;m just looking at the database now. But we can check. So we can see how much we write recycle on the database, right? So we can check the log lsn, that was written wait for five or 10 minutes, and then check the logs LSN, and us just do the math, to see how much data where we are writing to the database on own batch, we choose the wal size that we want, like, I want 30 minutes of wal, I want one hour I can afford? I don&rsquo;t know, half a day so there&rsquo;s not really a strict definition for what is the best. So the best is, what is the best for your workload for your case. So maybe you cannot afford your database to go offline for five minutes. You have no other strategy, this is the only database that you have. And if something happens, you need to recover from the crash in Max five minutes. So your wal file cannot be larger than it takes for five minutes worth of data. Right?</p><p><strong>Matt Yonkovit:</strong><br>With that. I mean, that assumes a single box. If you have failover set up, would you risk a larger wal file size and longer recovery? If the database does crash?</p><p><strong>Charly Batista:</strong><br>You can but then you need to pay close attention to your application. Because if your application is lagging, and you want to switch over from your private application, you still need to wait for your application to go up to the speed right to recover from the lag. Right. So you need to be careful with the high availability, especially because most of the recommended setups on Postgres are asynchronous, there are synchronous setups there, but like it&rsquo;s not the subject of this talk. So let&rsquo;s assume you have an asynchronous replication. That&rsquo;s the most common one, then you need to really take in consideration the risk, if your application is lagging a lot behind me, the time that takes for your application to get up to this week, and you promote is larger than your primary to just read from the old files.</p><p><strong>Matt Yonkovit:</strong><br>Most people will have that automatic failover, though, right. So if your replication is lagging, you could failover and your primary could recover quicker than your replication can catch it. Yeah,</p><p><strong>Charly Batista:</strong><br>That&rsquo;s true. That&rsquo;s true. That&rsquo;s why we really need to close eyes on those if you really, really want higher, high availability with levels of high level of SLA, the replication is something that really needs to keep eyes on it, it shouldn&rsquo;t lag too much. Because the switch over is really fast. Usually, people think the longer that takes on the process is the switch over, actually switch over can doing in seconds, or milliseconds depending on the latency of your network and configuration. So it&rsquo;s really fast. What takes long, is if your replicas is lagging behind of the primary, that can take quite a long time. And then your SLA, oh, five nines is just going down to the road.</p><p><strong>Matt Yonkovit:</strong><br>So I don&rsquo;t know if you were gonna check the lsn numbers. But I do notice that they will go back down a little. I do notice that your transaction time, like as you were talking, it just keeps on going up into the right. Which is, which is good, if that&rsquo;s a metric of how much users or money you&rsquo;re making, but not so good. If it&rsquo;s the amount of time that transactions are taking. That tends to be bad when it goes up.</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s indeed, that&rsquo;s indeed. So well, up to this point, there&rsquo;s not much we can do. Right? So we did a we were forced into the checkpoint. So I gotta wait for the checkpoint, I&rsquo;m not going to change to check the lsn because well, we need to wait at least one or five minutes. And as we&rsquo;re taking too long. So what are you going to do is I&rsquo;m going to use one gigabytes, because I did the math before. We can show it later, probably next session, or if we have time by the end of the session, but it just, it&rsquo;s a simple select. So just select the current LSN. So wait a minute</p><p><strong>Matt Yonkovit:</strong><br>What about synchronous commits or wal sync method as other ways to potentially reduce IO?</p><p><strong>Charly Batista:</strong><br>It&rsquo;s a trade-off?</p><p><strong>Matt Yonkovit:</strong><br>Well, I mean, everything&rsquo;s a trade-off. Right? From a performance perspective.</p><p><strong>Charly Batista:</strong><br>Exactly. Exactly. It&rsquo;s a trade-off. Again, it depends on your workload, can you afford that, like a lot of applications, for example, they need to use serializable transactions instead of read committed because they cannot afford any ghost reads or anything, right. And they should afford to lose some performance. To get a consistent view of the data. This is the same for the write, so for the commits. And there is even more strict that we can go like, for example, all the replication, we can use synchronous replication doesn&rsquo;t need to be a synchronous replication, right. And even on the synchronous replication sides, you can have the synchronous replication when you just wait for the replica to acknowledge that received the data and didn&rsquo;t apply it or you can wait for the replica to receive the data apply, and then go back to you and then you send back to the application, the result if it&rsquo;s success or failure, right. So, all of those can drastically reduce the stress that you put on your IO subsystem can improve the high availability level, the level of high availability that we can give to our customer, but definitely they have caught the performance can drop significantly. Like for example for the replication things. So we now need to wait at least the round trip to the network round trip. So if we have the primary in New York and the replica in Dubai, for example, the natural round trip can can can cost quite a lot, right. But if they both are on the same data center using fiber and really expensive switches and all these kinds of things. So sometimes the network round trip is cheaper term they are able that we have here. So all those things play in consideration, right. So, there is no really a formula to tell what is right or not. But all of them we add everything every time that we add something or for improve its scalability or to improve safety or wherever we&rsquo;ll probably add in on the performance problem. So you drop on the performance. Right?</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;s a trade-off. So you get better availability or better recoverability at the expense of performance.</p><p><strong>Charly Batista:</strong><br>Exactly.</p><p><strong>Matt Yonkovit:</strong><br>So, you&rsquo;re waiting</p><p><strong>Charly Batista:</strong><br>Yeah. See the checkpoint? He&rsquo;s taking 99%?</p><p><strong>Matt Yonkovit:</strong><br>Yeah. Well, so I haven&rsquo;t IO bound workload as well, if you want to look at it.</p><p><strong>Charly Batista:</strong><br>Yeah, let&rsquo;s go.</p><p><strong>Matt Yonkovit:</strong><br>Well, your checkpoint on your workload is over, they&rsquo;re doing its thing. If you take a look at the 130 box, on the PMM instance, that I sent there&rsquo;s an IP 130 Because I didn&rsquo;t name them nicely, like Charly did, is I&rsquo;m just a lazy turd. When it comes to naming things correctly. I believe that everything should just be an IP address name. I am not a fan of anything else. I just want like hardcore IP, six values and just remember them off the top my head, right.</p><p><strong>Charly Batista:</strong><br>Okay, I just need to I just need to learn how to use my browser.</p><p><strong>Matt Yonkovit:</strong><br>Yes. Looks like yes, browsing is hard. citizen or hot internet is hard.</p><p><strong>Charly Batista:</strong><br>When you take too long using the internet. I don&rsquo;t think if it&rsquo;s zoom or something, but it doesn&rsquo;t. Okay. I got it back. Oh, cool. F 11. Oh, I need to remember my username and password, right. That&rsquo;s something that I don&rsquo;t. Okay, let&rsquo;s try to Okay, I got it.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, 12345 is not a secure password.</p><p><strong>Charly Batista:</strong><br>Well, there are many stars on those, I can tell you for sure. 12345. At least up to nine.</p><p><strong>Matt Yonkovit:</strong><br>The same as my luggage.</p><p><strong>Charly Batista:</strong><br>My password is encrypted is the other way around is 98765431.</p><p><strong>Matt Yonkovit:</strong><br>Ah, so we do have a request to do some tuning on Read-Only workloads. Which Yeah, we can actually do that we&rsquo;ve got I&rsquo;ve got some some some workload, we can change over. The ones we&rsquo;re looking at right now are kind of a mix. So there are two workloads that I&rsquo;ve got for Charly here. One is a kind of a 5050 mix for web workload. And this one is an if you look at the 130 bucks, this one is a much heavier right workload. And this one&rsquo;s going to be more of almost like an archival with some analytics. if you will. Why did you break it? Choose the database? Choose database? Choose database.</p><p><strong>Charly Batista:</strong><br>Oh, I thought they just moved selected service.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, I don&rsquo;t know. It&rsquo;s, it&rsquo;s kind of weird.</p><p><strong>Charly Batista:</strong><br>But let me just put here to the last 15 minutes. Okay.</p><p><strong>Matt Yonkovit:</strong><br>But know that that&rsquo;s a good one. So Charly are you&rsquo;re up to 375 live streams now.</p><p><strong>Charly Batista:</strong><br>cool.</p><p><strong>Matt Yonkovit:</strong><br>One a day. Would it everybody show up if we did one a day? Like or two a day or maybe we just have Charly-like work lives on. On Twitch on YouTube. That could be fun.</p><p><strong>Charly Batista:</strong><br>Oh, we are on Twitch in Okay, that&rsquo;s my first one.</p><p><strong>Matt Yonkovit:</strong><br>So your first Twitch experience you even</p><p><strong>Charly Batista:</strong><br>Yeah, I&rsquo;m the beauty in there. Okay, cool.</p><p><strong>Matt Yonkovit:</strong><br>Yes, yes.</p><p><strong>Charly Batista:</strong><br>Well, I see that you have no shared buffers here. All right. Well, yeah, I can. Yeah, I can tell that your configuration is it&rsquo;s not cool. I was about to say a model word, but like it&rsquo;s not.</p><p><strong>Matt Yonkovit:</strong><br>Oh, no, you&rsquo;re gonna call me aged again. Oh, Yeah. Oh, like, I don&rsquo;t know what I&rsquo;m doing.</p><p><strong>Charly Batista:</strong><br>Yeah, it was because of this 128 megabytes for the 90s, you know?</p><p><strong>Matt Yonkovit:</strong><br>So here&rsquo;s the thing. This was a box that was set up for you to configure in tune, but then you set up your own box last time, so I never went back and did any changes. So this is pretty much default out of the box.</p><p><strong>Charly Batista:</strong><br>Yeah, I can tell. Okay, yeah, well, we have a lot of things to do. Right, we need to change the memory, we need to change kernel settings, the things that we did last time. Okay. Your checkpoint is quite cool.</p><p><strong>Matt Yonkovit:</strong><br>I don&rsquo;t know. What checkpoint cool. I mean, I guess it&rsquo;s kind of cool. Because it is very uniform.</p><p><strong>Charly Batista:</strong><br>Yeah, it&rsquo;s pretty. It&rsquo;s pretty.</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;s pretty it is. Yeah. Yeah.</p><p><strong>Charly Batista:</strong><br>See? And it&rsquo;s like, yeah. Well, we definitely see, you really have is like standards here. Do you mean patterns, I think we see patterns every see from year to year as well, here to order. So those are the checkpoints. If you see here, everything goes through the transaction increase increases, then checkpoint and goals and checkpoints. And if you look around your graphs, you&rsquo;re gonna see the patterns, they&rsquo;re all around your checkpoints. Right. So and definitely when we increase the memory, the shared buffers, we can improve that, because we&rsquo;re going to have buffer. So we can keep more data before moving them. Okay, where do you want to start? So I already told you, the shared buffer is not good, the checkpoints are not good.</p><p><strong>Matt Yonkovit:</strong><br>Do you want to recycle it and change those? Do you want to go in and change them? You can? Yeah, this is your box to play with?</p><p><strong>Charly Batista:</strong><br>Okay, so I suppose again, I have access, right?</p><p><strong>Matt Yonkovit:</strong><br>I did give you access last time. So, I did ask you prior to this. Look, oh yeah, your checkpoint failed. PG bench port. My workload is not PG bench. By the way, everyone. Although it does, I can make some wild and wooly changes if we want. If we want to make the workload really random. But right now it&rsquo;s running a uniform workload of so many writers and so many reporters, we can make changes. And if someone would like to come on and play Dungeons and Dragons Postgres with me sometime, I&rsquo;ll be the Dungeon Master. And I&rsquo;ll be changing variables behind the scenes, and then you have to figure out what I changed. It should be awesome.</p><p><strong>Charly Batista:</strong><br>I have sudo, okay, now I can restart the database.</p><p><strong>Matt Yonkovit:</strong><br>No! don&rsquo;t restart the database. It took me forever to get it started now</p><p><strong>Charly Batista:</strong><br>All right. So okay, I am checking your, your graphs here. I see from the database that you need to at least start with the basic settings, right, this is the base that we do when installing any samples. So base configuration for memory, or shared buffers, base configuration for wal. And those are the base settings. Right? So this is pretty common, actually, that we have, especially when we have like a setup that is done by developer. So we should not expect the developer to go deep into configuration because all this is not there for you. Right? Or if not that there are not developers out there that understand a lot of the database things I have some peers on somebody that they are developers, and they do understand a lot about database. But this is not their field. So this is DBA responsibility. Usually, when we get a database like these usually set up for a developer or it was a bad environment. And then for whatever reason it was promoted as production, so we get really out-of-the-box installation. So everything is default. The first things we check are those configurations, the most basic settings configure tuning, we shouldn&rsquo;t even call the tuning but anyways, those are the first thing that we should check right. Take a look at the memory configuration. So shared buffers, how the caches been used, if the box has or not. Configuration for swap how the CPU, Governor, all those basic things to make at least, the database to perform up to the box, let&rsquo;s put on this way not to deal with the keyboard configuration that we have. Right. So. But then we need to know, like, for example, how much memory we have on this database we can use like just I did here and see that Matt doesn&rsquo;t have swap at all, which is bad. And we&rsquo;ve discussed why it&rsquo;s bad last time who should have</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;s the defaults when you spin up an instance? It&rsquo;s default?</p><p><strong>Charly Batista:</strong><br>Yeah. Yeah. So don&rsquo;t stay on defaults then argued there. So. And I would say a better approach, or to take a look is to use the tools that we have, like for example, the PMM. So I can here on my node, I can have a node overview, and should have been opened here. Let me choose. its upgrade. Alright, have this one. Yep. It&rsquo;s not the right one. So and I want actually, what I want to see is on Node overview on the summary, okay. So we can get most of the information that we need from here, right? So for example, how much memory we have. So I can see that we have 32 gigabytes memory on this box. So I can see what is the swap message that you want? That&rsquo;s good thing, even though we don&rsquo;t have swap available? I can see the configuration folder as kernel like the dirt caches and things here. We can also see, take a look on the IO that you should have somewhere around here. What is the IO scheduler, so you don&rsquo;t have rage. So that&rsquo;s common, especially on AWS. Because they do not expose this information from from the VM level. You have transparent, huge pages enable here. This is not good, either. Okay, let&rsquo;s start from here. You have 32 gigabytes of memory. Right? So usually, for this recommendation, so out, I&rsquo;d say like, if you go up to 12 gigabytes of memory in this box, it&rsquo;s not bad. Remember, I do not know your workload. Can you do it right now? Well, I&rsquo;m looking for the problems that workload is causing, not your workload. Oh, right. So this is different. I&rsquo;m looking at the problems he calls it, I see that you have how many connections, so not that many. So usually less than 100. Last 100 connections, there&rsquo;s not bad, but I have no idea what those connections are doing. So I can see that you what kind of activities, the read activities here. So not as bad and your transactions are taking quite long. So, I can have some hints of your about your workload, but I don&rsquo;t really know your workload in I only know what your workload is calls into the database, right? And this is this is the premise.</p><p>So usually, because Postgres doesn&rsquo;t do direct IO, and it relies a lot on the kernel to do buffer IO, we usually don&rsquo;t give too much memory for Postgres. There are exceptions. And for example, if you really have a really read intensive workload, it might be a good idea to give more memory to Postgres, then to the cache, because possibly scan can cache better information based on the tables and the data that you&rsquo;re using, though as always caches blocks, right? So kernel doesn&rsquo;t care. Those are just blocks that it got from the file system. And it&rsquo;s caching. So the cache on the database is more optimized. It&rsquo;s more, it&rsquo;s a specialist question. Let&rsquo;s put on this way, because it knows the data, it&rsquo;s caching. So there are some circumstances where it&rsquo;s better to have more memory for the database to OS cash, but they may try it off the workloads, or put a 60 75% of their workers I&rsquo;ve seen, it&rsquo;s better to keep more memory to the OS cache down to the database cache. And that&rsquo;s why I usually start with smaller value, like, for example, eight gigabytes, 12 gigabytes, when you have like a box less than 100 100, gigabytes memory, right? So it&rsquo;s not formal. Like we have all the data is like MySQL, they usually recommend you&rsquo;re giving 75- 80% of the memory available memory to the database, right? It&rsquo;s not like this on Postgres, you can give like, up to 40%, if you don&rsquo;t have a lot of memory on the cache, but as the amount of memory on the box increases, 40% can be a lot like if you have 100 gigabytes memory on this box, that will be 40 gigabytes memory. It&rsquo;s a lot. It&rsquo;s sometimes it&rsquo;s most of the time it&rsquo;s better to have both cash for the Kernel cache than to the database. Anyway, too much talk, let&rsquo;s start changing. So I need to know the configuration user you&rsquo;re using? And you both, I guess that&rsquo;s it, etc.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, it&rsquo;s under Postgres. 13. Main. Yeah. And I think it&rsquo;s going to be pg_ident.conf</p><p><strong>Charly Batista:</strong><br>Actually, this is using usually is using this one. Okay, there you go. But we can, we can check later on Postgres. Just because I&rsquo;m too lazy. Now. I gotta go here. So see, this is where we have your data directory, file, so we don&rsquo;t grab all those. Listen address? Well, your workload so far, you haven&rsquo;t complained about the number of connections. So we&rsquo;re going to keep the 100.</p><p><strong>Matt Yonkovit:</strong><br>That&rsquo;s because I limited the number of connections, I could add more. But yeah, I knew I had a limit of 100. So I didn&rsquo;t go over. And you aren&rsquo;t your pool or bouncer, you are my you&rsquo;re my client.</p><p><strong>Charly Batista:</strong><br>So I want to keep 100. All right. Alright, so let&rsquo;s go for the shared buffers, okay. I got to start very conservative, let&rsquo;s start with six gigabytes, right? There is no magic. There&rsquo;s just because I want you to give it I start with six gigabytes. Remember, I have to restart the database. This is one thing that you need to keep in mind. So now it&rsquo;s fine. Like our, my customer didn&rsquo;t say that I couldn&rsquo;t, I could not restart the database that well. So I&rsquo;m taking it for granted that I can restart the database without problems, right. So if we need to increase, you can increase. But if you&rsquo;re working on production, be careful too, because you need to restart, like the database. So I got a small for now, four. megabytes for work_mem should be fine. And let&rsquo;s do just this one, let&rsquo;s let&rsquo;s just books. Only one, change the shutter buffers and see how that can impact in your load. Right? So I&rsquo;m not going to change anything else, only the shared buffers. And we&rsquo;ll see how impactful this is a very good approach when we&rsquo;re working with tuning and optimization do not change too many parameters at the same time. Because if something goes well, you have no idea which parameter causes the improvement. If something goes really bad, you also have no idea what parameter get our database in bad performance, right? So every time that we going for 30 or something, change one parameter, test it out, see how it works, how it behaves, then we&rsquo;re going to understand what the parameters does. So I should suppose, have you ever used? You did use it here? So I&rsquo;m restarting your database your as my customer, you&rsquo;re fine with that Matt? Okay, thank you. Shall I ask for a written approval? Yes. Okay. Well, I restarted.</p><p><strong>Matt Yonkovit:</strong><br>Did it restart?</p><p><strong>Charly Batista:</strong><br>Yeah, I got a check now. It says it&rsquo;s alive and I can connect to the database.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, that&rsquo;s weird though, because the app didn&rsquo;t recycle.</p><p><strong>Charly Batista:</strong><br>See? That&rsquo;s a problem with developers. Sometimes they take things for granted, you may need to restart your application as well. Right?</p><p><strong>Matt Yonkovit:</strong><br>Maybe, let&rsquo;s see. It should have</p><p><strong>Charly Batista:</strong><br>I know that my client doesn&rsquo;t have a connection pooling so that may be a problem when he restarting database.</p><p><strong>Matt Yonkovit:</strong><br>I don&rsquo;t know. Did it actually restart or did it do like a just an act to return?</p><p><strong>Charly Batista:</strong><br>No, no, it&rsquo;s restarted. Well, it&rsquo;s, I can do a stop. Let me stop here. Right?</p><p><strong>Matt Yonkovit:</strong><br>Postgres still has workload on that box.</p><p><strong>Charly Batista:</strong><br>So it stopped. Now it stopped.</p><p><strong>Matt Yonkovit:</strong><br>All right, hold on. Do it top real quick. Cuz I&rsquo;m still seeing tons of Postgres. Just just just look at the processes real quick. Yeah, so we&rsquo;re Yeah, so it didn&rsquo;t start okay.</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s true. System D.</p><p><strong>Matt Yonkovit:</strong><br>By the way, this is the box that you set up last time</p><p><strong>Charly Batista:</strong><br>We created swap and we change a lot of things last time, do you remember?</p><p><strong>Matt Yonkovit:</strong><br>Oh, maybe the other one is then maybe 111 is</p><p><strong>Charly Batista:</strong><br>Okay, let&rsquo;s give it some time. Do you want me to do something and tell people never do that?</p><p><strong>Matt Yonkovit:</strong><br>No.</p><p><strong>Charly Batista:</strong><br>Oh, because i can see you have a lot of movie JSON user connections here. Okay, let&rsquo;s spin theme things up. Right. So people that are watching never do that, especially in production. Really? Never ever do I&rsquo;m going about to do here. That&rsquo;s said.</p><p><strong>Matt Yonkovit:</strong><br>Anyway, it stopped now. It&rsquo;s done.</p><p><strong>Charly Batista:</strong><br>Yeah. Whoa, almost. Okay, I&rsquo;m not going to run. I just keep it here for posterity. We never know. Okay, yeah. You&rsquo;ll have some processes running. Yeah, Postgres is still running.</p><p><strong>Matt Yonkovit:</strong><br>Well, that&rsquo;s, oh, no, here it is.</p><p><strong>Charly Batista:</strong><br>Yeah, so I got to just send a secure signal. So this is fine. This one is fine to send a CQ signal to Postgres and the other process, because it doesn&rsquo;t forcefully kill them. Just ask them can you please stop? Sometimes they replied, No, we don&rsquo;t want to stop. And usually because probably is doing some OS activity. See this guy here? See the D is in to the reputable estate. It&rsquo;s doing IO. And probably everybody&rsquo;s waiting for this guy to finish his IO. And then it stopped.</p><p><strong>Matt Yonkovit:</strong><br>this one was heavily IO bouncer. So are you saying that the reason that it&rsquo;s not stopping is because it&rsquo;s waiting for the checkpoints to clear to stop?</p><p><strong>Charly Batista:</strong><br>Yeah. But I wonder what happens if I forcefully kill them. Remember that we told Postgres is able to recover, right? Are we able to back up our words here? Okay, this one, this one is fine. This one is for the PMM. So, I forcefully killed Postgres, right? So now, Postgres is in an inconsistent estate, because it was still doing some IO, whereas when they stopped the database, it still has a, especially if you have a really IO bound workload. So even when we stopped the database, there are a lot of things in memory that the database needs to flush, the kernel needs to flush. So there&rsquo;s a lot of activity that the database needs to do. That&rsquo;s another thing that you need to take into consideration when you have a lot of memory and you give a lot of memory to the database. Sorry, they start up in the shutdown process, they might take longer because they still need to flush or that all that are IO and they need to finish up some a lot of operations that they&rsquo;re doing background, right. And at this point, the database is an inconsistent state because well, I just killed if q dash nine.</p><p><strong>Matt Yonkovit:</strong><br>Shame on you.</p><p><strong>Charly Batista:</strong><br>Now my fault, I asked my customer, he said yes.</p><p><strong>Matt Yonkovit:</strong><br>No, I never said kill. I said you could stop it.</p><p><strong>Charly Batista:</strong><br>I did stop them. So it says Postgres, it says it&rsquo;s active at this exit. Okay. Let&rsquo;s try to connect. It&rsquo;s not really running. Anymore. Now we have a problem. Postgres didn&rsquo;t really start. So what do we do now? I hope you have logs on Postgres. Right. Yeah, I gotta check the configuration. Okay, yeah, we do have log, and it&rsquo;s the inside of the data dir folder lock. Okay. And the data G I saw something here it is. Yeah, this one. So we should go for this one. And we should have a log folder. Inside of this folder. We should have logs and which days today?</p><p><strong>Matt Yonkovit:</strong><br>The 11th. Okay, last one there should be it right.</p><p><strong>Charly Batista:</strong><br>Yeah, let&rsquo;s take a look. All this is the shutdown one. Is not the one we&rsquo;re looking for. It should be this one. Wow. That&rsquo;s interesting. See the timestamp?</p><p><strong>Matt Yonkovit:</strong><br>Yeah, those are from earlier today.</p><p><strong>Charly Batista:</strong><br>Yeah. It didn&rsquo;t log they attempt to start the database.</p><p><strong>Matt Yonkovit:</strong><br>Okay, so systemctl might not be working right? That&rsquo;s possible. Yep. Or it might not be called Postgres SQL Server. It could be something else.</p><p><strong>Charly Batista:</strong><br>That could be</p><p><strong>Matt Yonkovit:</strong><br>I think in this box, I thought I saw you monkeying around with PG up there PG start a PG start script. Did I? I thought so.</p><p><strong>Charly Batista:</strong><br>Okay, let&rsquo;s let&rsquo;s do Oh, yeah, that&rsquo;s indeed</p><p><strong>Matt Yonkovit:</strong><br>Yeah, because you were you were playing around with this before when we were doing some work on the previous stream, and if I recall correctly, on this particular box</p><p><strong>Charly Batista:</strong><br>okay, let&rsquo;s see why. Okay, this is the one</p><p><strong>Matt Yonkovit:</strong><br>Yeah, you&rsquo;re using PG Start. Prior PG start, stop.</p><p><strong>Charly Batista:</strong><br>Okay, yep. Okay, now we&rsquo;re back. Yeah, the system G is having issues here. That&rsquo;s unfortunate. I like system G. So we have</p><p><strong>Matt Yonkovit:</strong><br>Charly, user. Yep. Just user. That&rsquo;s all.</p><p><strong>Charly Batista:</strong><br>Sorry Matt. What did you say?</p><p><strong>Matt Yonkovit:</strong><br>I said that was just user error. We just forget the config earlier, I think when we were doing the last stream. Yeah, when we were shutting it down from the last stream. And we&rsquo;re done with it.</p><p><strong>Charly Batista:</strong><br>Yeah, I&rsquo;ll take a look on that one later.</p><p><strong>Matt Yonkovit:</strong><br>But the shared buffer is now up to eight gigs.</p><p><strong>Charly Batista:</strong><br>It&rsquo;s yeah, it&rsquo;s six</p><p><strong>Matt Yonkovit:</strong><br>Sorry.</p><p><strong>Charly Batista:</strong><br>Yeah, I was gonna start but I wanted to save two gigs for later. So, you spinning up the load again?</p><p><strong>Matt Yonkovit:</strong><br>Yep, give me one second.</p><p><strong>Charly Batista:</strong><br>Let&rsquo;s see if it was anything.</p><p><strong>Matt Yonkovit:</strong><br>The load for this is Charly heavy, right workload. That&rsquo;s the name of this workload</p><p><strong>Charly Batista:</strong><br>you name it your workload, really?</p><p><strong>Matt Yonkovit:</strong><br>I mean, after Charly it is Charly&rsquo;s workload so blame his workload.</p><p><strong>Charly Batista:</strong><br>Let&rsquo;s take a look at this one. And I suggest we do if we just finish from here because I think we are more than one hour Right? Right. So we can keep going from here to the next session so to finish see it may not be able to finish everything in one session but you may have your 25 by the end of the day.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, well I think I think efficiently you&rsquo;re up to 793 live streams you owe the community here so that&rsquo;s what I mean</p><p><strong>Charly Batista:</strong><br>all right. Let&rsquo;s take a look so my box if it&rsquo;s still waiting for the oh no the checkpoint is done. Thank God I didn&rsquo;t crash my box.</p><p><strong>Matt Yonkovit:</strong><br>The workload on mine is running it&rsquo;s just gonna take a few minutes to start like doing it stuff right</p><p><strong>Charly Batista:</strong><br>Yeah, well my workload it&rsquo;s back so what is my suppose this is my box there are too many boxes you know I&rsquo;m confused Yeah, this is my box. See, it took just 15 seconds for this transaction to finish almost not slow it took a lot longer was there how many transactions that took at least 15 seconds, and see how many locks we had during that time. all right. So now we are back and we should see the IO wait everything is back as we didn&rsquo;t change anything or fluctuation on my box right. So one thing that I remember is that we want to change the wal size, the min and max wal size. So as we are advancing on time, I going to do it here. And okay, I don&rsquo;t need you. I need to restart my database. So let&rsquo;s stop this load here. I&rsquo;m using Patrone on this database here. And I&rsquo;m also using port 22. Do my day my configuration changes. So everything that I change it on my configuration, they are here you can see. So I keep the max wal size is full. And you&rsquo;re right, it&rsquo;s eight megabytes of min_wal_size. So what we&rsquo;re going to do is, we&rsquo;re going to increase for one gigabyte, of my min_wal_size. And for this box are going to use five gigabytes of my max wal size. And this is not enough, we need to tell the database how we want the database to distribute that load over the time that we were given to the database, because while even though we have like a max wal size of five gigabytes, we can also have the the timeout for the I just forgot the name the checkpoint, right? So we need to give the database more time to process I don&rsquo;t want five minutes timeout. I think that&rsquo;s the default for the configuration. And I really don&rsquo;t remember if the name of for me to SharePoint time out on top of my head now.</p><p><strong>Matt Yonkovit:</strong><br>I&rsquo;m googling timeout the timeout parameter.</p><p><strong>Charly Batista:</strong><br>Yes. I&rsquo;m just using Google it&rsquo;s easier. Yeah, it&rsquo;s checkpoint timeout.</p><p><strong>Matt Yonkovit:</strong><br>Oh, yeah. So checkpoint.</p><p><strong>Charly Batista:</strong><br>The checkpoint timeout. I going to give it 3600 seconds, right. So it&rsquo;s in seconds if I&rsquo;m not mistaken. And I want to give it one hour. I need to tell the database, how it can proceed. Because while even though I tell the database all the time out for the checkpoint for this five gigabytes here, it&rsquo;s, I want you to split the load evenly with the time I want you to wait more to wait less. So this is what we call the completion target on Postgres. There is a change on Postgres 14 If you&rsquo;re not sure not wrong, that, they put it hard coded to 0.9. So it will try to wait as much as it can to finish the checkpoint. It won&rsquo;t force the checkpoint to promptly finish. So and it&rsquo;s the name of the parameter is a checkpoint. Now I googling again, because they don&rsquo;t really remember</p><p><strong>Matt Yonkovit:</strong><br>You mean, like the completion target or?</p><p><strong>Charly Batista:</strong><br>Yes, the completion target. Yeah.</p><p><strong>Matt Yonkovit:</strong><br>I mean, the default is point nine. It&rsquo;s point nine. The default is point five. No. Now, as of 14. it is point nine.</p><p><strong>Charly Batista:</strong><br>It is okay. Sure. Oh, I&rsquo;m using 13 by the way. I&rsquo;m not sure if they put the full point nine.</p><p><strong>Matt Yonkovit:</strong><br>it changed in and it&rsquo;s point nine in 14. And it&rsquo;s point five in 13.</p><p><strong>Charly Batista:</strong><br>Yeah, I&rsquo;m using 13 here. So I got to put point five. The only thing that we change is related to the wal right, so I&rsquo;m checkpoints nothing else. Gonna save. Oops, I need to be rude to do this. systemctl restart Patroni. Yeah, it should take some time. Because we have a lot of things going on here. So it should take some time to restart the database. That&rsquo;s expected. So you got your load back? Yes. Okay. While you were waiting for my database to get back, let&rsquo;s go back to your workload. You&rsquo;re shredding it back. Okay. Yeah, I was just ALRIGHT.</p><p><strong>Matt Yonkovit:</strong><br>I got it back. Yeah,</p><p><strong>Charly Batista:</strong><br>I can see I can see that. No, that was my bad Okay, let&rsquo;s see what happened. Last week, we changed your checkpoint behavior. Button, just reload this here. It doesn&rsquo;t look, right Okay. Am I connected to the right place? Yeah.</p><p><strong>Matt Yonkovit:</strong><br>130? Yeah,</p><p><strong>Charly Batista:</strong><br>130. 130. Yeah. So? And is this last 15 minutes, right? Since we change it, you&rsquo;re now you&rsquo;re writing more to your database. Let&rsquo;s just take a look on those. See, now we are doing more checkpoints. How is that possible? What happened was not supposed to improve your load?</p><p><strong>Matt Yonkovit:</strong><br>Come on you once you free one bottleneck, the bottleneck shifts, right? I mean, so when you&rsquo;re reading off of disk, it competes for the checkpoint. So it&rsquo;s yeah, it&rsquo;s interesting. So if you look at like the worry throughput, the number of queries per second doubled. And the response time really stabilized. Although the response time is a little higher, it looks like. No, no, it dropped down, it dropped down quite substantially. Things are warming the cache.</p><p><strong>Charly Batista:</strong><br>So now we have double the throughput right? requests per second. Yeah, that&rsquo;s good. So my customers getting happier. Because now we can do double the job at the same time, right?</p><p><strong>Matt Yonkovit:</strong><br>Okay. Double the job, and the IO wait went down quite substantially. I mean, now you&rsquo;re in. But keep in mind, this is also warming up still. So after a restart, you won&rsquo;t get too aggressive checkpointing until after the workload has been running for a while, right?</p><p><strong>Charly Batista:</strong><br>Well, you are getting very aggressive here, with a lot of checkpoints.</p><p><strong>Matt Yonkovit:</strong><br>Well, but you&rsquo;re not as backed up is what I&rsquo;m saying is like, over time, it what you&rsquo;ll see after a restart things can perform at a certain level. And then as system buffers start to fill up as you start to run out of space and get more aggressive in having to do background operations, then things start to slow down. So it&rsquo;ll be interesting because let me go look, I&rsquo;m going to look at the last six hours. Yeah, so if you go if you look at like the last six hours of IO wait, for instance. Okay, so last, yeah, last six hours, just Yeah, six hours. And look at the IO wait, numbers like CPU, IO await. You can see like the first you see around 1500. You see that workload? They&rsquo;re like 1500 o&rsquo;clock. So you&rsquo;re too early with your thing? You got to move it back to them? Yeah, right there. You see, you see the jump there. So it ran fine for whatever that was maybe like an hour. And then after that hour, then it almost doubled the io wait. So the workload ramps up and it takes a while. And so for that first hour, things might look hunky dory happy in terms of IO. And then after, but actually that&rsquo;s more like 15 minutes, isn&rsquo;t it? Maybe maybe 10 minutes. So then after a certain amount of time, bam, right, like IO just like spikes up. Which is interesting, because it&rsquo;s that warm up time. And it&rsquo;s kind of pushing everything. So I&rsquo;m wondering after a certain amount of time here, if we&rsquo;ll see a spike again, with that IO, especially considering a lot of these are just very basic.</p><p><strong>Charly Batista:</strong><br>No, we probably especially because the only thing that we did we change it was we gave more memory for the database. And actually, I&rsquo;m expecting that the ramp-up going to be earlier than later.</p><p><strong>Matt Yonkovit:</strong><br>Because of the disk because you&rsquo;re now freed up. Yes, regular queries to start pulling things quicker.</p><p><strong>Charly Batista:</strong><br>Yes. So and that&rsquo;s when it comes to the parameter that we just change it on my database, right. So now we will We have to wait. And that&rsquo;s why my recommendation is okay, we wrap up for today,</p><p><strong>Matt Yonkovit:</strong><br>right, he said for you wrap up for a year. So you don&rsquo;t want to do live stream.</p><p><strong>Charly Batista:</strong><br>Here, here. Oh, I know I have really bad English, and on my pronunciation</p><p><strong>Matt Yonkovit:</strong><br>Your year and I like what he wants to wrap up for the year.</p><p><strong>Charly Batista:</strong><br>And we keep from here next session, right? So we like I&rsquo;m not going to touch on my box. I&rsquo;ll leave it as it is. Right? So then we have the full picture of what we did. That&rsquo;s tiny little change. So next session, we can start the load like 1530 minutes. Well, even before the</p><p><strong>Matt Yonkovit:</strong><br>So if you have time we can even just like whatever time you want to get on next week, just this random like, Hey, I got an hour, I got a half-hour to look at this again, we can jump back on.</p><p><strong>Charly Batista:</strong><br>Sounds good.</p><p><strong>Matt Yonkovit:</strong><br>I mean, because we don&rsquo;t need to announce that it&rsquo;s coming because most people are gonna watch off. Anyway. So</p><p><strong>Charly Batista:</strong><br>that&rsquo;s fine. Because it&rsquo;s getting really lengthy is like one hour, one and a half hours, right. So you just</p><p><strong>Matt Yonkovit:</strong><br>want to go to your weekend. It&rsquo;s Friday, everybody wants to go on their weekend. I want to go to my weekend. We all want to go for our weekend. It is late. We should let this be fun.</p><p><strong>Charly Batista:</strong><br>it&rsquo;s raining dogs and cats here. There&rsquo;s nothing much that I can do all there, you know?</p><p><strong>Matt Yonkovit:</strong><br>Yes. All right. Well, so we did see some improvement here. Like I said, I&rsquo;m interested to see after the next hour, so yeah, whatever to see what the load on my site changes, probably. But we can come back and just let things run and see what happens.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s the thing. So let&rsquo;s we should let it run well, and it&rsquo;s been applied like one hour earlier. Next time, we get some time just to warm it up and see how that goes. And then we go back. And we compare we change the other parameters and give some time to run again. And as we have different workloads, you have to have one so we can change going forward between them while we are talking.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, absolutely. Absolutely. Absolutely. And we can also throw in that read-only workload that we had somebody ask about as well.</p><p><strong>Charly Batista:</strong><br>Yes, yes. That&rsquo;s also a good one. And that&rsquo;s interesting to compare how they differ from each other. Right? Because yeah, one thing is right intensive workload, we have some type some problems that we see here. We might have total different types of problems.</p><p><strong>Matt Yonkovit:</strong><br>absolutely. All right, everyone, thank you for coming along. Hopefully, you learn something this week. We&rsquo;ll try and do just a random kind of like, stream next week to kind of close out this topic. Before we get to the next topic in our lists, which I believe the next topic after this is starting to dig into replication setup. So we were going to do a basic config. And then we were going to go jump into replication setup and getting replicas. And how do we monitor lag and things? So we do want to get that to there as well. Oh, look. Nando has been hanging out with us as well. Oh, and Nando you were so quiet this whole time we didn&rsquo;t even know you were there. You&rsquo;re just you just lurking lurcher.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s the wrong guys.</p><p><strong>Matt Yonkovit:</strong><br>All right. All right, well, we&rsquo;ll catch you next time then. We appreciate it. And check out our live streams coming up over the next few weeks and we&rsquo;ll catch you next time</p><p><strong>Charly Batista:</strong><br>Yeah, and again, if you have questions or topics that you want to discuss here, just say no, right? You have Matt&rsquo;s social media so you don&rsquo;t have time because I&rsquo;m not so social.</p><p><strong>Matt Yonkovit:</strong><br>But we&rsquo;re gonna make Charly social. By the time I&rsquo;m done with him. He&rsquo;s gonna be he&rsquo;s gonna be mini-HOSS. All right. Bye, everybody.</p><p><strong>Charly Batista:</strong><br>Bye, everybody.</p><h2 id=transcript-part-02>Transcript Part 02</h2><p><strong>Matt Yonkovit:</strong><br>Yeah, we are coming live, everyone. Oh, we got echo. That&rsquo;s horrible. There we go. Oh, now no echo, no no echo. Hello, everyone, and welcome to one of the live streams. This is a very special live stream here. We didn&rsquo;t finish last week as we were going through some workload. And we decided to come back and take a look at what&rsquo;s going on on our two Postgres servers that we had set up last week to see what we can do to make some modifications and some tweaks. So yay for us. So we&rsquo;re excited about that.</p><p><strong>Charly Batista:</strong><br>Yeah. Hey. Okay. Hey there, Matt. Yeah, we really left last week with some optimization. Right. So actually, we started taking a look on a server. So if I might share my screen? Yeah. I was just logging into your server here to see what we have. This is what they had from PMM. So we have some load, right?</p><p><strong>Matt Yonkovit:</strong><br>Oh, my, we have some load? Yes, we do. Yes, we do. And, and, and look at that beautiful, non-uniform distribution.</p><p><strong>Charly Batista:</strong><br>Yep. Let&rsquo;s get just the last three hours to see here.</p><p><strong>Matt Yonkovit:</strong><br>Look, look, Gonzalo joined us. Hey, Gonzalo. Well, this is a stream we didn&rsquo;t figure many people would stop by because we didn&rsquo;t announce it on purpose. This is a follow-up to kind of finish our live stream from last week, and probably be good to put the two recordings together. So. But I do have a new hat to show off when before we&rsquo;re done. So that&rsquo;s important. I visited our esteemed CEO today, and he brought me a new hat. My dogs think it&rsquo;s a chew toy, though. So this is my new hat. We&rsquo;ll put it on in a little bit. To build the suspense to keep the audience watching us. Right, we want to have them pay attention as much as possible to the stream. So stay tuned for the hat reveal. It&rsquo;s coming up.</p><p><strong>Charly Batista:</strong><br>So today is not only my suffering here to figure out the problem, your database, right. So we have this hat</p><p><strong>Matt Yonkovit:</strong><br>My suffering.</p><p><strong>Charly Batista:</strong><br>Yes, no, this is the life, your order, the customer goes there and do something like I know you probably did something.</p><p><strong>Matt Yonkovit:</strong><br>You know, me, why would I do something? Oh, why would I do something?</p><p><strong>Charly Batista:</strong><br>All right. Let&rsquo;s start. So what were we left?</p><p><strong>Matt Yonkovit:</strong><br>Well, we changed the shared buffers last time. That was the big change. Yes, we did change , and we saw a significant increase in overall performance.</p><p><strong>Charly Batista:</strong><br>Okay. So what you&rsquo;ve seen during the week, and your load test, do you still have this performance improvement?</p><p><strong>Matt Yonkovit:</strong><br>So it is still performing better? But we are seeing some errors now.</p><p><strong>Charly Batista:</strong><br>Okay, which are?</p><p><strong>Matt Yonkovit:</strong><br>We&rsquo;re actually hitting connection limits.</p><p><strong>Charly Batista:</strong><br>Oh, now we are hitting connection limits. But as I remember, you had a 200 connection, right?</p><p><strong>Matt Yonkovit:</strong><br>I believe we only had about 100. If we go look at the top, I think Max connections was set to 100 on this box.</p><p><strong>Charly Batista:</strong><br>Yeah, yeah. Yeah, we can check it.</p><p><strong>Matt Yonkovit:</strong><br>So you can see like right below you see the Postgres connections. You can see where it hits over. 100 Right.</p><p><strong>Charly Batista:</strong><br>Yeah, it does. That&rsquo;s yeah. Sometimes, tries to go over a limit. And this is interesting how it is possible if you have a 100 connection limit, the graph here shows you 200.</p><p><strong>Matt Yonkovit:</strong><br>This is a good question. Tell us, Charly.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s a good grasp. Right. So how is that possible?</p><p><strong>Matt Yonkovit:</strong><br>So, tell us, Charly,</p><p><strong>Charly Batista:</strong><br>that&rsquo;s my question, right?</p><p><strong>Matt Yonkovit:</strong><br>There are idle connections, right. So most of them are said</p><p><strong>Charly Batista:</strong><br>Yeah, the total number of connections here, see, we have like 89 actives, some idle transactions. And it should not go over the limit, right? It should not go over 100 connections, because it told the database to limit 200 connections, right? So what happens if I go to the database? Now? Let&rsquo;s connect here</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;ll probably let you through, by the way, because I don&rsquo;t think we&rsquo;re at 100. right, this second.</p><p><strong>Charly Batista:</strong><br>Okay. So if I, yeah, right now, they start activity just show like around 50-52 connections. So why do sometimes do we see this over-limit number of connections. Still my question any gas, and eat?</p><p><strong>Matt Yonkovit:</strong><br>Any hint or guess, so you&rsquo;re making me pick? Again, I told you, of course, that I don&rsquo;t like that. I want you to guess where yourself. So there are actually a couple of things that I can think of number one, it can be a metrics issue. So because of how PMMs collect metrics, it could be counting metrics that are between cycles, right? So, you might see at the point in time, where it collects the Postgres connections merged with a second or third pole, and they might show is different. Right? So, that is a possibility. Sometimes, you&rsquo;ll see what we call metrics jitter and some metrics where you&rsquo;ll average two numbers together, and it will not make 100% sense. So that is a possibility.</p><p><strong>Charly Batista:</strong><br>Especially for databases, like Prometheus and Deterra metrics. Sorry, it sometimes happens quite often. Yeah, that&rsquo;s one of the possibilities. And the other thing that you could</p><p><strong>Matt Yonkovit:</strong><br>well, if you look at the connections, next, you see the max connections actually do pass their stop at 100. So we are only getting up to 100 active connections at one time. This means that the other connections that are being listed are not active.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s too, and we have. Another thing that we need to take into consideration is this math comes from when a connection buys for the endpoint, what we have for client-server architecture, we should have two endpoints, right, we have the client, and we have the server. So we have the Postgres client and the post Server database. So from the TCP point of view, when you open the connection, everything goes fine. So, we have all those two endpoints, when you close the connection, they don&rsquo;t immediately close the front from the connection point of fill. Right? So, you still have it on all those endpoints, the connection alive for a certain period of time. It depends on how we collect the metrics, it can count as a still open or active connection, or from the database point of view. So even for the Postgres, that connection is now active, and more but still alive from a TCP stack point of view. Right. And this is important to notice and understand, especially when we have a proxy in front of the database. Let&rsquo;s say we have a PG bouncer, if we have a configuration problem from the Postgres side or if we have certain timeout limits, too short on the database side and database, the post we start closing the connections by itself, those quadruples, there is a client TCP IP client, TCP port, server IP and server port. They&rsquo;re going to be busy, usually for like two minutes. That is the timeout for the limit stack to release them. If we put if we have too many closing connections from the database site. And they all come in from the proxy. Eventually, we&rsquo;re going to reach the man Number of the connection that this allowed for the Linux kernel to accept from that node, in this case from the proxy. So even though we might not have any live open connections from the database, then the proxy will not be able to connect to the database anymore, because it&rsquo;s been restricted by the kernel. It&rsquo;s a common problem. Like, for example, websites when we have something in front of your website, like a proxy in front of Nginx, or Apache. So it&rsquo;s something that can happen quite often, especially for configuration problems. So if you have a misconfiguration, it can also happen to the database, right? And when we see something, why these it is worth, investigating if the problem was just about metrics, or if the problem is just what I tell you, it might have a misconfiguration problem here. Okay. Right. So we might have like something from your database causing connections, because the problem is, who closes the connection? If the application closes the connection, so is the application that needs to wait for the kernel for the OS stack to release that is spot on the connection timeline. If the server closes the connection, then on the server-side, it has worked. So and we might have this implication, right, let&rsquo;s keep just a note for now. Because this is not what we are for today. Today, we want to look at performance.</p><p><strong>Matt Yonkovit:</strong><br>Well, all configuration. And obviously, if users can&rsquo;t connect, they have no performance. You can&rsquo;t deny me</p><p><strong>Charly Batista:</strong><br>Whoa, depends. While, if my database has no queries, it&rsquo;s quite fast. I can tell you,</p><p><strong>Matt Yonkovit:</strong><br>But users are what matters. Right? Users?</p><p><strong>Charly Batista:</strong><br>I agree. I tried to escape that one, but</p><p><strong>Matt Yonkovit:</strong><br>You can&rsquo;t escape that. Can&rsquo;t escape that.</p><p><strong>Charly Batista:</strong><br>Alright. Yeah, but let&rsquo;s keep if it&rsquo;s a note. So this is something that works investigating. Right? Yeah. But right now. So if we look at the application, you&rsquo;re saying that we&rsquo;re, the application is hitting the limit? The number of correct, right. I don&rsquo;t think it&rsquo;s doing right now, because I can&rsquo;t see the graphs.</p><p><strong>Matt Yonkovit:</strong><br>Yeah. So it was at a certain point, and then it backed off here. Yeah. Because what users couldn&rsquo;t connect to, they said, Screw you, guys. We&rsquo;re going to a different website. That&rsquo;s what they said. Yeah, that&rsquo;s what happened. They all hit they couldn&rsquo;t get on. And they went away.</p><p><strong>Charly Batista:</strong><br>Which is, which is bad? That&rsquo;s poison. It&rsquo;s bad. Or okay. Yeah, it&rsquo;s not good. All right. So let&rsquo;s check here. How is Oops! Okay. So while I&rsquo;m here on the database, I want to check some parameters, right. So usually, the most common way that people go is to do like something like show all, and check all the parameters here.</p><p><strong>Matt Yonkovit:</strong><br>Right, a lot of parameters though, isn&rsquo;t it?</p><p><strong>Charly Batista:</strong><br>Oh, they do. They do. We can also go here, on PMM, right! So we can, you can have something similar here. On the settings here, so this is a little easier to read. Yeah, I was about to say the same. It&rsquo;s a lot better to read. Right. So and we can have some, some settings here that we usually the most we can put here. Okay. So as a client, would you like to increase the number of connections? So, you wouldn&rsquo;t have the same problem again?</p><p><strong>Matt Yonkovit:</strong><br>Yes, I want you to put it to 100,000 because I never know how many I&rsquo;ll need.</p><p><strong>Charly Batista:</strong><br>I would love to put in 100,000. Like well, Buddy, I have like a wild gas that would not work with work quite well, for your setup to 100,000. Correct. What? It&rsquo;s just a wild gas. I might be wrong, but it&rsquo;s just a way of the gas. Right? Because, well, you have a 32-gigabyte machine right or a gigabyte box. So let&rsquo;s say we have a 32 gigabytes box. If I remember, right, yes, this is 32 gigs. Okay, so we have</p><p><strong>Matt Yonkovit:</strong><br>by the way, are you going to do math? Now I know, no. No math. Okay. Just gonna say if you&rsquo;re gonna do the math, I was gonna shut my camera.</p><p><strong>Charly Batista:</strong><br>I was just I just want to say that we have some limitations for the number of connectors, but we have some things that they put physical limits for, for max number of connections, right? So for every connection that we get to the database, we need to use some portion of memory. Right? So then if you have 32 gigabytes of memory, or in your box, you need to have some memory available for those connections for each of those individual connections, so they have some buffers, like, for example, sorting buffers, the query buffers, all sold both buffers, they gonna use memory. Right? So, but you also have an operating system on your box, don&rsquo;t you?</p><p>I mean, it&rsquo;s just a database.</p><p>No, yeah, full. And unfortunately, the operating system also needs memory. So we cannot use all the memory that we have for the database. Another problem, but this is the most common thing that people use to tell the users they cannot increase the number of connections because they have a limited number of memories. Right? But what if your database had 1.5 terabytes of memory Ram, would you put, would you be able to put like that 100,000 connections? One thing? No, let&rsquo;s say you have 1.5 terabytes, right?</p><p><strong>Matt Yonkovit:</strong><br>Wait, let me put on my thing. That&rsquo;s quite a lot of memory. Right. What am I thinking, cap? So do you think the answer is no, I don&rsquo;t think that that is possible? My thinking cap, says, hit some limits there. And by the way, if I had 1.5 terabytes of memory, I would have to be filthy rich. And so I would pay someone else to figure that out for me.</p><p><strong>Charly Batista:</strong><br>Yeah, I don&rsquo;t know. But the thing is, some people like to do those, even though they have plenty of money to pay for someone else. They like to be bold and do things. And there&rsquo;s nothing wrong with that. If you like, you&rsquo;d like such a person that you&rsquo;d like to get your hands dirty, doing things. That&rsquo;s fine. Oh, well, I probably would also pay for someone else to do that thing for me</p><p><strong>Matt Yonkovit:</strong><br>I knew that you were like me. Ah, yeah. So I mean obviously, the memory constraints at that many connections, and just managing that many connections are going to be too many. And I mean, any database is going to start to fall over dead pretty quick. As you start to open that number of connections. It&rsquo;s not just the memory that&rsquo;s constrained at that point, you&rsquo;ve also got CPU concerns, and you&rsquo;re going to have weight conditions and other things that happen. So obviously, there is some limit, and that&rsquo;s why connection pooling. So critically important, especially considering that your active throughput is going to be limited by the number of cores and the number of things that are going to be active anyway. So unless you&rsquo;re going to match that 1.5 terabytes with a 10 million core box, you might not get the results that you want.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s a very important thing.</p><p><strong>Matt Yonkovit:</strong><br>Of course, it&rsquo;s important because I wear my elephant hat, and it totally told me that that was right. Elephants are wise, right? Or is that oh, yeah,</p><p><strong>Charly Batista:</strong><br>they are there. And they have a really good memory. Right? Of course, they have a good memory. So yeah, and that&rsquo;s, that&rsquo;s a very important thing. CPU. So when we&rsquo;re going for the database, we have a lot more to do with other than just memory and IO, right? I&rsquo;ve seen people talking about IO, how badly it can impact performance. And again, again, they&rsquo;re right, yeah, IO can really be a huge problem when you have IO saturation, and I, and we&rsquo;ve seen on the last talk, how badly can impact performance. Right. But well, we also start to get into the CPU saturation, it can be really, really bad as you said. Ah, we have the physical limitation, as it doesn&rsquo;t matter how fast the CPU is, the CPU is still can only do one job at a time. Right? So one CPU core can only do one job at a time. Right? So and if we get to that number of connections, let&rsquo;s say the 100,000 connections, so let&rsquo;s put we can put, I don&rsquo;t know 1000 cores, 1000 CPU cores in our, in our box, well, doing really simple math, every core needs to deal with at least 100 connections at a time. Right? So, that would put out a huge pressure on the CPU. And it will start to go really, really slowly. Do you want to do something?</p><p><strong>Matt Yonkovit:</strong><br>What do I want to do? You can play around with this box. That&rsquo;s fine. I just have to restart the workload.</p><p><strong>Charly Batista:</strong><br>I just want you to change here. It&rsquo;s a mistake. It&rsquo;s only to see PostgreSQL. Okay. 13 main. Postgresql.com. I want to put here to 2500 2500 You are a madman. But I don&rsquo;t want you to put 2500 connections, you just want 1000. But it was the database to be allowed to accept 500. All right. So let&rsquo;s start slowly, just with 1000. If I don&rsquo;t make the mistake, we need to do a manual restart here, right?</p><p><strong>Matt Yonkovit:</strong><br>Yes, there was something wonky with how that one was set up. We go back and connect the service CTL.</p><p><strong>Charly Batista:</strong><br>Yeah, we yeah, we can do it later. But right now. So what am I going to do? Okay, this is the start. And okay. Do we have a stop here?</p><p>No. Is there any problem sending a cue to all those signals on Postgres?</p><p><strong>Matt Yonkovit:</strong><br>Well, we talked about that before, it will have to recover from them. So it is not a recommended best practice to kill.</p><p>We covered that in the last one because you had to kill this instance before we didn&rsquo;t really fix this instance or change over to the other instance that is not kind of wonky?</p><p><strong>Charly Batista:</strong><br>Well, that&rsquo;s fine. I just want to show you that we do have different signals. And we said, yes. Oh, yeah. I want to just send a CQ. Right. I don&rsquo;t want to send it because when we stopped the data is when he stopped Postgres, actually what it does is sends a CQ to the database. A CQ is is a gentle request to the database that just asked the database, can you please stop? Right? So actually, this is what the system does when I do the system D, Postgres whatever service is stopped, it will send a CQ to that service anyway. Because well, the service might be doing something during that time. And eventually, it will just stop, right? This is what I want to do here. I want to do a CQ, I&rsquo;m going to do a kill-9 because a kill-9 is it&rsquo;s a huge problem. And they got to ask you why a kill-9 is a problem for the database</p><p><strong>Matt Yonkovit:</strong><br>Is it pulling the rug out of everything? Like underneath everything, it just our core crashes and shuts down. Right. It&rsquo;s like stop. Don&rsquo;t wait for anything. Don&rsquo;t wait, just wipe it clean.</p><p><strong>Charly Batista:</strong><br>Yeah, why is that a problem?</p><p><strong>Matt Yonkovit:</strong><br>Because there are still things in flight being written. There are operations that may be kind of in a wonky state. You want to kill as gracefully as possible or shut down as gracefully as possible.</p><p><strong>Charly Batista:</strong><br>Yeah. Yeah, that yeah, that&rsquo;s, that&rsquo;s true. So for databases. Remember, we have a lot of caches on the database, right? For example, in this instance, we gave the database, six gigabytes of cache. If we&rsquo;re writing to the database, every time that we do a commit, we do an update, and whatever operation that writes to the database, it does not immediately write down to the disk. It might be in on some sort of cache for the database, right? And when we do a kill-9, as you said, it doesn&rsquo;t wait for anything. It just cuts off the edge, and it&rsquo;s gone. So if we have any cache or anything in the cache that still needs to go to the disk or needs to be asked to the OS to be flushed. It won&rsquo;t have time to do so that&rsquo;s why we always want to do to send a gentle CQ. to say it didn&rsquo;t do immediately because it&rsquo;s, it was doing its own cleanup. But now it&rsquo;s gone. You don&rsquo;t have here that the Postgres anymore, right. So that was a gentle ask the database. As you see here. There are some services they were doing some cleanups and some stuff that&rsquo;s still needing to be processed. And when it was done, the database just found that it was time to go and rest in peace. So that&rsquo;s why it&rsquo;s so important to do not to send a kill-9 to forcefully kill the database unless it&rsquo;s extremely necessary.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, last resort because the recovery is questionable. And so if you do a kill-9, there might be data loss or corruption or some other fun. Well, fun things happen. Actually, if you want to have a very eventful and exciting day, go ahead and kill dash nine of your database. Yes, everyone out there. If you&rsquo;re looking for some fun this weekend, go out to production and do a kill nine. And tell them Charly told you to.</p><p><strong>Charly Batista:</strong><br>Well, and I can even show him. Yeah, they can take the just this would be the nice comment. And we&rsquo;ll kill everything that has Postgres. Right? Don&rsquo;t do it. Don&rsquo;t do it. But don&rsquo;t do it. It&rsquo;s just</p><p><strong>Matt Yonkovit:</strong><br>Don&rsquo;t do it. Don&rsquo;t do it.</p><p><strong>Charly Batista:</strong><br>All right now.</p><p><strong>Matt Yonkovit:</strong><br>So is it, did you start it? Is if you started on it.</p><p><strong>Charly Batista:</strong><br>Start, I&rsquo;m starting now. Yeah, I&rsquo;m just now, let me take a look here. Where is my instance? So okay, it&rsquo;s everyone may not, I want to be a bit faster. Let me put here the last 15 minutes. So it can see it better. Yep, we have an interruption. Just here. And it&rsquo;s back. It should be back. It is back. And here we have hiked. See some odd here? There you go. see we have we are up to almost 3000 connections. So what is the max connection that you put for your old?</p><p><strong>Matt Yonkovit:</strong><br>I can put it as whatever you want. You want to see a lot of connections. Yeah. Wants to see a lot of connections. How many do you want to see?</p><p><strong>Charly Batista:</strong><br>Alright, like, I think 1000 It&rsquo;s, it&rsquo;s fine. You have a big box</p><p><strong>Matt Yonkovit:</strong><br>Yeah, the only problem is you&rsquo;re gonna run out of resources on the app server. Because I&rsquo;m running a single app server right now. But this is actually a very easy change for me to do. Let me go ahead and</p><p><strong>Charly Batista:</strong><br>Just want to know what happens if we have 1000 connections?</p><p><strong>Matt Yonkovit:</strong><br>Okay, fine. Just give me a second. You are so impatient. Has anybody ever told you that your impatience is not fun?</p><p><strong>Charly Batista:</strong><br>Well, I Well, honestly, probably markdowns.</p><p><strong>Matt Yonkovit:</strong><br>Okay, so how, why don&rsquo;t I just go to, like, 150 or so real quick. And then we can adjust from there. How does that sound? Right? Because like I said, I&rsquo;m not sure how much the app server that I&rsquo;m using can spin up. Python threading.</p><p><strong>Charly Batista:</strong><br>Limits, right?</p><p><strong>Matt Yonkovit:</strong><br>Yeah. Yeah, I mean, this is where like, from a Python perspective or your application server, there&rsquo;s also going to be a limit there on how much can be thrown to the database, as well. And I mean, you get around that by building this out, like a Kubernetes, or a cluster or a containerized application where you can add more nodes. And that&rsquo;s fine. I can do that as well. It&rsquo;s just I so happen to have a single node right now that&rsquo;s running his workload, which causes some other issues as well. But I&rsquo;ve been bottleneck gain this on purpose. So yeah, so right now, it should be that the number of connections should be increasing to at least a couple of 100.</p><p><strong>Charly Batista:</strong><br>And what problem that we have with Python is because it was not designed to be mute core, right. So even if you create 1000 threads in your Python unless you&rsquo;re using C Python,</p><p><strong>Matt Yonkovit:</strong><br>I&rsquo;m using processes, so these are spinning off individual processes. Okay. So yeah, so each one should be able to handle, but it&rsquo;s still gonna have some weight things, but you can look, those connections are increasing.</p><p><strong>Charly Batista:</strong><br>It is. So we have more than 100 connections open here, and now it went down.</p><p><strong>Matt Yonkovit:</strong><br>That&rsquo;s probably because, let&rsquo;s see. Yeah, it&rsquo;s probably because this workload randomized, so it will continue to change and evolve over time. But here, just Yes, I want to overwrite it. Because yeah, that&rsquo;s what happened. So now it should go back up to those connections in a few seconds here, it&rsquo;ll bounce back up. There&rsquo;s actually just a configuration file that has the number of connections for each type of workload you want. And I just randomly change that every five minutes or so to adjust the workload, so you get these nice graphs that go up and down. Because one of the biggest problems you see with benchmarking is when you have a consistent workload, it is a false workload, right? No one runs consistently at whether it&rsquo;s 80%, 100%, or 60%, you see spikes up and down based on what&rsquo;s happening in that timeframe. So I figured every five minutes randomization will work. I can also run it, so it is parallel. So one of the other databases we have up here has been running the exact same workload for a week, and you will see almost no deviation from its parameters, right? It&rsquo;s just kind of consistency across the board may be plus or minus, like a 5% change. So that&rsquo;s one of those really interesting things that you have to be mindful of and avoid. But look at that, look at your look and see just for you, Charly, I generated more, more and more.</p><p><strong>Charly Batista:</strong><br>Nice. So</p><p><strong>Matt Yonkovit:</strong><br>Everyone, Charly asked for more connections, just like we need more cowbell and most songs.</p><p><strong>Charly Batista:</strong><br>And my question is, do we have actually more tuples with more connections?</p><p><strong>Matt Yonkovit:</strong><br>Probably not. I mean, I think that there&rsquo;s a couple going to be a couple of limiting factors here, right? So because even though these are processes on Python, you&rsquo;re going to have the application server still has to receive the results and do some processing, which is going to take some CPU cycles as well. So in this case, by adding more connections, you could also be hitting your CPU limit on the database server, which means that more connections are just going to flood the gates here. So, take a look at what&rsquo;s the CPU right now. I mean, I&rsquo;m guessing that&rsquo;s going to be pretty high. Yeah, I mean, you&rsquo;re hitting 100%, right? Yes, no, you are maxing out to CPU. So adding another 1000 connections here isn&rsquo;t going to do anything.</p><p><strong>Charly Batista:</strong><br>And this is an important, really important realization, especially when it comes, to development. Right. So it&rsquo;s a really common part, the development teams, they say, look, the application is it is low, because well, the database is behaving is slowly can we have more connections to the database. So we open more connections, we have more processing in parallel on the application side. And I&rsquo;ll, it&rsquo;s pretty common that we do not realize that&rsquo;s open more connections doesn&rsquo;t mean that more data coming from the database, we just have more connections waiting for the data from the database side, right? Yeah. And as we are putting more and more load, the database is going to like the results are going to be stressed and saturated now, instead of being faster, or slower. Because we have a lot more saturation.</p><p><strong>Matt Yonkovit:</strong><br>Yeah. Yeah. And I mean, if you go to real quick, go under like the dashboards to go to a custom dashboard. So if you go to the site and hit managed, so like the side menu bar, yeah. So go up one. Then go to manage. And scroll down. Believe Yeah, so there&rsquo;s a PMM test Postgres only dashboard. Click on that guy. Yeah. Now that&rsquo;s MySQL, click MySQL, go to MySQL. I have one for all that all databases that put these together, but the Postgres one alone is designed and optimized just for Postgres. Yeah. Yeah. So this is where if we look in your on 111, so you&rsquo;re gonna have to change the node to 130. We should be able to look at the response time here. And so we get this response time graph. It&rsquo;ll be interesting to see. If you go back the last hour if you notice any sort of like improvement or degradation in the response time itself. So you can see that there, there&rsquo;s a more sustained spike in that kind of six milliseconds. And we did get a pretty big jump in the number of queries per second, though, look at that. I mean, we&rsquo;re almost at 30,000. So we are processing more, but it is taking a little longer to process it looks.</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s the thing. And there are a lot more than a number of connections to improve performance. Right. So if we try to put more, also, we can be floating your network?</p><p><strong>Matt Yonkovit:</strong><br>That&rsquo;s another big question. Yeah. I mean, I&rsquo;ve seen that before where the network pipe, the bandwidth between two servers just gets absolutely saturated and bogged down.</p><p><strong>Charly Batista:</strong><br>Exactly. And if you take a look here, see, if at some point it starts going down? Because this is what happens? Okay, we&rsquo;re starting fresh now. As we start thrashing, we don&rsquo;t have the cache to buy. So we take some time to warm up the cache. And eventually, as we have too many connections, so many transactions at the same time, the cache became too small. So we never actually get to award cache, because it&rsquo;s, it needs to be replaced too often. Right, right. So that can also be another problem when we have too many connections. And one thing that can happen is when we have a connection pooling in front of the database, is also to have an external caching system like Redis or Memcached. Fridays. Yeah, yes, exactly.</p><p><strong>Matt Yonkovit:</strong><br>is anybody still using mem cache out there? I mean, I was when I was around. It was. That was the thing, right? I mean, that was kind of progress, though. But kind of my dating myself. Yes. I&rsquo;m so old. They do. Like to Grumpy Old Men. Charly. Did you, by the way, did you see that? The new stream cover? For our stream? If you haven&rsquo;t, you can take a look there. Let&rsquo;s see. So what do you think? I thought we should caption it. Too Grumpy Old Men talking Postgres because we look kind of sad there. Right. I mean, I don&rsquo;t know what is. that&rsquo;s, I think it&rsquo;s, it&rsquo;s an interesting cover there for us. Definitely. Oh, yeah.</p><p><strong>Charly Batista:</strong><br>We have a pool. Right. So.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, yeah, we can have a poll. We can have lots of polls. I don&rsquo;t know if we, I mean, like, so&mldr; Yeah. I mean, obviously, there are other things we could do so this is the other one that the team came up with? I don&rsquo;t know. What do you think of this one? Chino and the HOSS here? We can definitely?</p><p><strong>Charly Batista:</strong><br>Yeah, I&rsquo;m open to those ideas and new ideas.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, so Chino and the HOSS. All right. So anyway, back to our beloved graphs and charts. Now, now that we&rsquo;ve taken that outside, we could play stump the Charly later on.</p><p><strong>Charly Batista:</strong><br>Look how the latency is going here. And how the available memory is going down here. Right. Yeah. I think that the machine is getting quite big red?</p><p><strong>Matt Yonkovit:</strong><br>Yeah, there&rsquo;s a lot of red on that, that that machine, they&rsquo;re in their late, like, look at that, right? It&rsquo;s just like, like, like, the CPU. It&rsquo;s like, ah, if blood-red, it&rsquo;s like bleeding out of its eyeballs.</p><p><strong>Charly Batista:</strong><br>It is. Well, I quite like your dashboard here.</p><p><strong>Matt Yonkovit:</strong><br>Wait, wait, hold on. I just need to bask in that. At that moment that Charly quite likes my dashboard. Everyone. The stream has ended. There is nothing more I can do. I have made Charly like my dashboard. And he gave me a compliment. He didn&rsquo;t try and stump me. Oh, I&rsquo;m done. Microphone drop. I&rsquo;m done. I&rsquo;m done. I retired.</p><p><strong>Charly Batista:</strong><br>You know, that&rsquo;s not good for my image</p><p><strong>Matt Yonkovit:</strong><br>That difficult to deal with that, that I sit there, boom, I know. So, Charly, this was actually built with the support engineers and who actually gave me details on what to look forward things. So I added everything that all the support engineers to this dashboard. I personally think this should be the default dashboard. I mean, I&rsquo;m just going to tell you that right now that I&rsquo;m like, this should totally be it right here, and it should just be done like this. Should be the default for Postgres. But I do not get that say. But if you&rsquo;re watching this later on, and you&rsquo;re like, that&rsquo;s an awesome dashboard, it is available on GitHub right now. So I can point you in the right direction, all this wonderful stuff, including the thing that&rsquo;s making this workload, is out there as well.</p><p><strong>Charly Batista:</strong><br>Yeah, and this is one thing that I really like about PMM. It&rsquo;s extensibility. Right? So it&rsquo;s easy for anybody out there to get it and make amazing features and backward like this one. It&rsquo;s very extensible. One can say, oh, but it doesn&rsquo;t have everything. Well, maybe that&rsquo;s the trick. It doesn&rsquo;t have everything. But it&rsquo;s open for anybody to implement anything that they want.</p><p><strong>Matt Yonkovit:</strong><br>You see some cool,</p><p><strong>Charly Batista:</strong><br>You can pull your coffee machine here and get data for you</p><p><strong>Matt Yonkovit:</strong><br>But you want to see some cool, scroll up? Yep, scroll up to my query view. Okay, find a query that you want a little more information on, you might have to scroll to the right to get like the metrics. Great. Yeah, like, there you go. So if you wanted to look at like the one that has the highest average time or max time, like choose one, just yeah, look at this. See. So this is where you can get the heat chart for that one particular query, the response time just for that one query, how many queries per second that&rsquo;s running. And if you scroll down, you can see visually, how each one of those queries ran and get a nice little kind of like, histogram of that. So boom, mic job. Anyway, that was not why we were here. So Charly, back to tuning this instance. Okay, but thank you for the praise, I feel accomplished. I am retiring as King of the PMM, dashboards, and King of streams. Thank you all for participating. Charly, we&rsquo;re here to see what else we can do. Obviously, the workload has changed again. So that things have adjusted, as I said, users will come and go on the system. And during the busy times, you saw, we got up to like 300 connections. But like any other real production workload, you&rsquo;re going to have spikes. And you&rsquo;re going to have things that drop. Right. And I think that&rsquo;s one of the challenges of any sort of tuning exercise, is as you look at workload, what was happening five minutes ago, might not happen again, for another day, as I in that up and down sort of mentality means that having the ability to go back and kind of drill into any one point in time and see what was causing issues is really important.</p><p><strong>Charly Batista:</strong><br>It is. it is. let&rsquo;s take a look here. Well, we see this period of time here that the database had quite a lot of connection</p><p><strong>Matt Yonkovit:</strong><br>Bazillion I think it&rsquo;s a bazillion. I think that&rsquo;s the official metric for the number. It had a bazillion queries running.</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s accurate math busy. Yes, bazillion. Like,</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;s very close to Brazilians, but not quite the same.</p><p><strong>Charly Batista:</strong><br>Well, yeah, exactly. Not quite the same. So let&rsquo;s take a look here.</p><p><strong>Matt Yonkovit:</strong><br>It&rsquo;s okay. Charly is from Brazil. So I can say Brazilian? Because he&rsquo;s Brazilian. Yeah. So the Brazilian is looking at bazilians of the query. It&rsquo;s fun, it&rsquo;s like a dad joke. Right? That&rsquo;s still fun. It&rsquo;s a horrible joke. Okay. Oh,</p><p><strong>Charly Batista:</strong><br>You got me laughing? That&rsquo;s not the that&rsquo;s Yes. Yes. So let&rsquo;s, back here to do our job. Right. So one interesting thing that we can take a look at is when the load started increasing, actually, the number of tuples went down. Yes. Right. So, if you see here, we have a lot of tuples. When we have a lot of more active connections, well, looks like the data that&rsquo;s been out from the database did not correspond to the increased number of your connections. Right? So and we need to investigate why. Right? So usually, you want to look at the OS at the very same time, the same period of time. Right. So, okay, we did. we should have. Okay, let&rsquo;s take a look. Oh, come back. He&rsquo;s just in the field a lot. I want to see here my node summary. And I want to select oops, that is not this one. So let&rsquo;s make sure that we have exactly the same. That one here and yeah, so we&rsquo;re looking at the very same time range, right.</p><p><strong>Matt Yonkovit:</strong><br>By the way, Charly, I think that if you go back to the way you had the original time range, you can right-click on the node&rsquo;s summary and open up a new tab that will keep those consistent numbers.</p><p><strong>Charly Batista:</strong><br>It would, it would be okay.</p><p><strong>Matt Yonkovit:</strong><br>I just wanted to let people know, wow, that had a load average of 63 on the box. By the way, did you see that? Oh, my gosh, that box is ready to fall over? Yeah, that&rsquo;s a pretty high load average.</p><p><strong>Charly Batista:</strong><br>Why is it a high load average? How can we tell if a load average is higher low?</p><p><strong>Matt Yonkovit:</strong><br>Well, so I mean, that load average is the number of runnable processes that are on the box, right? So or that&rsquo;s, that&rsquo;s available to be run. So if you&rsquo;ve got, let&rsquo;s say, eight cores, a load average of eight is probably going to be okay, maybe give or take a little bit. But if you&rsquo;re 10x, that means that there is naturally a bottleneck because you have more runnable things than can run at any one time.</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s quite good. Of course, it&rsquo;s good. Also, another thing that you need to pay a thing to keep in mind these on Linux, the low, how low the average is calculated is quite weird. It also takes into consideration the IO process running. Yes. Right. So if you go for another Unix system, it&rsquo;s most of them really take into consideration the process running. And like, for example, the process on an interruptible estate that&rsquo;s just waiting for IO, they are not usually not taken into consideration for a low average, this doesn&rsquo;t quite happen on Linux. Right. So I honestly don&rsquo;t know if it&rsquo;s good or bad. But we just need to also take those things into consideration.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, I mean, like, but just looking at these graphs, I mean so not only are we at 100% CPU, but you&rsquo;re saturated. You&rsquo;re totally consuming all the cores. Now, interestingly enough, that process graph has a rounded curve, too. And it&rsquo;s kind of interesting. Like, it looks almost like, as a planet&rsquo;s like part of a planet like a horizon. It&rsquo;s a really interesting graph. From a view perspective.</p><p><strong>Charly Batista:</strong><br>You&rsquo;re looking to the horizon, right? Yeah, it is. Yes. And we can start finding some answers here. Well, the first thing you said, All CPUs are saturated. Right. So definitely crashed the load average for our eight CPU cores of 63. It&rsquo;s not ideal. Definitely</p><p><strong>Matt Yonkovit:</strong><br>Not ideal. Let&rsquo;s be honest. It&rsquo;s like it&rsquo;s horrible. That&rsquo;s apocalyptic. It&rsquo;s horrible. Yes, yeah. Yeah. If you&rsquo;re at, like that much. Yeah, that&rsquo;s apocalyptic.</p><p><strong>Charly Batista:</strong><br>Yeah. So that&rsquo;s the first problem, right? So clearly, this box is, is not able to handle this node. Right. And here, you can see the context switching rise and up with something that we usually do not expect, when we have so many connections and the CPU is being smashed. We weren&rsquo;t we expect to see a lot more context switching. So usually, not all the time. But usually when I see this, this pattern here, from the box that smashed by huge remote. It smells like IO problems as well. So our CPU is starting to wait for IO. Things can also get quite bad. And if you see here, the IO latency, it&rsquo;s been crazy. See, we see up to 16 milliseconds, sometimes close to 20-30 milliseconds of IO latency. Right? So this is crazy. If we&rsquo;re thinking about like, you&rsquo;re asking for something to your own disk. Before you start getting anything back, you need to wait for 15 milliseconds. From the CPU point of view, it&rsquo;s like an eternity, right? So its date is never. Right. And that&rsquo;s why probably the main reason we don&rsquo;t see like so huge spikes here. If we take a look, when it&rsquo;s its increasing here. See, when the IO latency starts to increase, here, we see the number of context switching going down because now the CPU has nothing to do an audit and wait, why does it need you to switch and context? And when it&rsquo;s starting, get back? The data back from the IO then okay, CPU starts going, working quite a lot again, right? So this is interesting. So we have two main problems here. And at this moment, that huge number of connections didn&rsquo;t increase performance, because well, they are smashing our CPU. And they cause a lot of IO problems to our IO, right? So if you see here, actually the IO activity gonna slow down, because the I/O goes so high, it&rsquo;s so extreme that this doesn&rsquo;t process. Yeah, yeah. Ability to keep up with this. Right. So those are the results of what we did well, at this point. For this box without changing any configuration, so the better thing that we could do is to put a connection pooling like a PG bouncer or PG proxy in front of this box and limit the number of connections to the database. Right? So probably taking it back 200 150 Max connections to the database. Look at the TCP transmission here. It&rsquo;s also a bad indication. It&rsquo;s not that high. But like it was, it was lower before, right. So everything is starting to get messed up. And the network traffic is as expected as now we have 1000 connections, sending a lot of things to the database. We will get the network probably for that as well. Yeah, so for this investigation here. 1000 connections is definitely too bad, right? For two main reasons that we have. So but what if we put it back two to 500. Do we think that the box would be able to handle 500?</p><p><strong>Matt Yonkovit:</strong><br>Well, keep in mind, while we had 1000 connections, we only ramped up here to around 300. So if you look at the overall number of connections here, we didn&rsquo;t even hit the 500 or 1000.</p><p><strong>Charly Batista:</strong><br>That&rsquo;s a good point. That&rsquo;s a good point.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, see right there. So do you do see a couple of spikes that hit up to like 500, but it&rsquo;s very rare. Most of the time. It wasn&rsquo;t.</p><p><strong>Charly Batista:</strong><br>Yeah, mostly. Yeah. Most of the time. We&rsquo;re below 300.</p><p><strong>Matt Yonkovit:</strong><br>Yeah, so I only added about 300. Now again, I could throw 1000. But it was already dead.</p><p><strong>Charly Batista:</strong><br>Oh, no, we will read crash. Yeah,</p><p><strong>Matt Yonkovit:</strong><br>Yeah, it is unless we want to do that again. oh, no, it&rsquo;s Daniil. Welcome. Thanks for joining. Thanks for saying hi.</p><p><strong>Charly Batista:</strong><br>And well, that&rsquo;s what we saw from the last point right, now let&rsquo;s take a look at the database. So, okay, I just want you to go back to Okay, here is where we started. So, when we start getting a load that is too high, one thing that we should expect is to increase the number of locks inside the database, right? So, there are many types of locks on the database but the overall number of locks, in general, should spike in Yeah, this is so this is around when the time that the load starts, right? So, if you see the number of blocks, they&rsquo;re not that bad. And then things start getting quite high here, right.</p><p><strong>Matt Yonkovit:</strong><br>And if you compare that to now so if you draw out your graph and look now or look starting like just change to now. Just changed. Yeah, yeah. I&rsquo;m trying to. Yeah. It&rsquo;s loading, there you go. And so if you come down there, you can see like, once those connections went back to a normal workload because keeping in mind, we ramped that workload up to 300. Whereas normally, it was running in the let&rsquo;s say, 50 to 120 range on a normal given day. You could definitely see that change, you can also see that there&rsquo;s a workload change that just happened as well. So you see, like, so you can see that workload changes directly in that graph.</p><p><strong>Charly Batista:</strong><br>Yeah, that&rsquo;s pretty clear. So that&rsquo;s pretty clear.</p><p><strong>Matt Yonkovit:</strong><br>They look at, wow, we&rsquo;re finding out stuff.</p><p><strong>Charly Batista:</strong><br>And these impact, we could see how impactful the raising of the number of locks was to the throughput that even though we have like three, four, or five times the number of the connects that we usually have the throughput went down.</p><p><strong>Matt Yonkovit:</strong><br>a good way to see this is if you go back, to my tab. So go to the tab with my, yeah, there&rsquo;s one. And go ahead and refresh this real quick. Because yeah, we just want to see what the latest is. So we should see that, that kind of spike there. So go ahead and just highlight, let&rsquo;s say, the last since from 15-30. Yeah, the last hour should be good. So you see the spike in the DB connections up to that 300. If you go down and choose one of the queries, we can look at the histogram for that particular query and look at don&rsquo;t do an update, do alike select probably be easier, I think. Yeah, yeah, we can look at the throughput for though, or the latency for each of the queries. And you can see how long each one takes. And so we should be able to see that spike. Yeah. So you see like the spike started to go up with that</p><p><strong>Charly Batista:</strong><br>Response time here is increasing, it&rsquo;s crazy. Yeah.</p><p><strong>Matt Yonkovit:</strong><br>And then it dropped back down once those connections went away. So the more connections, the more bottleneck the slower things got. Exactly, exactly. But, yeah, I mean, you could see that in this as well, where you start to scroll down, and then like, look at that one, right there. You see that one that just like, right at the top, it was like, Oh, my God, this is ready to die. And then it came down. And that&rsquo;s, that&rsquo;s kind of like what your end users are going to experience. Right? If this is a query, that&rsquo;s something roughly quite frequently.</p><p><strong>Charly Batista:</strong><br>Exactly, exactly. And this is the point. Just increasing the number of connections doesn&rsquo;t make things faster. Yep. Yeah, and it can go the can goes backward, right, increasing rise into too much the number of connections can just degrade the performance. Horrendously. Right? As we, as we saw here,</p><p><strong>Matt Yonkovit:</strong><br>Yep. Okay, so Charly, we&rsquo;re about out of time.</p><p><strong>Charly Batista:</strong><br>Okay, I just said to you, oh,</p><p><strong>Matt Yonkovit:</strong><br>just one more thing,</p><p><strong>Charly Batista:</strong><br>You want to check the chat as well. Yeah, just a checkpoint here. If you see here, around that time, see how your checkpoints around like five minutes, then sometimes just, I just want to select. this is one thing that we can change in the configuration. And we can add users for your workload, we don&rsquo;t want a checkpoint of five minutes or every three minutes, right? So, we want the checkpoint to be more evenly distributed in your workload, because well, it can reduce pressure on IO, right. So, well, those here during that the big, the high number of connections that we have, we can clearly see how much more checkpoints how more frequently the checkpoints were, right. So, if we cannot reduce the number of connections, one thing that you definitely need to do from Postgres adjusts the checkpoints configuration. Because these we can reduce a little bit it won&rsquo;t make magic it, but it will help especially with the IO. Remember that when we saw the IO stat this one was somewhere here. Yeah, also got really smashed and the latency went horrible. So the checkpoints there contribute a lot to this problem. So if we change some settings related to the checkpoint, we can have just one section focus on the checkpoints and how we do that. I think that would be really good. So we could improve the problem. But you see I had a problem related to the CPU and a lot of IO when networking bandwidth came in, but like at least were tackling some problems that we some points that you can write. Yep. So this is one area that we definitely. another one that we just mentioned here, we won&rsquo;t have time to check today are about the vacuum. So the vacuum is a topic that deserves a whole talk when</p><p><strong>Matt Yonkovit:</strong><br>One entire vacuum talk.</p><p><strong>Charly Batista:</strong><br>Yes, so, but yeah, this is just another thing that we will probably need to take a look at. And can also help improve the IO issues that we had there. Yeah, those are all the last things that I wanted to share for today. I think we were on top of the hour. We discussed a lot.</p><p><strong>Matt Yonkovit:</strong><br>We did. We did. And so Charly, let&rsquo;s go ahead and flip your camera back to just you. And we will bring you back in for the reveal of the Peter hat. So I went to lunch with Peter, our CEO today and he told me that he bought me this hat. This hat is evidently made of rain. It&rsquo;s a ram hat. So this is like rain for something. So I&rsquo;m gonna put it on just for all of you, and he told me I needed to smell it because it smells like lotion. Well, the dogs get to chew toys. Oh, yeah. Oh, yeah. The dogs got really excited when I brought this toy. Oh, yeah. They were like they were jumping all over trying to get at it. Because this is like what you would like, they have these toys you could buy at the pet store. Like this, right? Yeah. It looks like a triple maybe from Star Trek so that they Okay, let&rsquo;s do the hat change. So you could still hear me, but I can&rsquo;t hear you for a second. So let&rsquo;s see. So, let&rsquo;s see. Evidently, I&rsquo;m supposed to get it as low as possible. It doesn&rsquo;t really work with the headphones very well. It actually looks like a wig. I don&rsquo;t know. Yeah. I mean, like, maybe other people can tell me this. This. This actually doesn&rsquo;t look like a hat at all. It looks like a wig. It looks like a fancy dress with this.</p><p><strong>Charly Batista:</strong><br>But before you put the headphones in there are some dresses, some hats, or Asia prior to Mongolia. And then that they have some hats there. It looks like those</p><p><strong>Matt Yonkovit:</strong><br>Do they Okay, all right. Well, I think so.</p><p><strong>Charly Batista:</strong><br>This camel flower. Something like that looks good.</p><p><strong>Matt Yonkovit:</strong><br>You know, that&rsquo;s cool. I&rsquo;m happy. Like anytime somebody wants to send me a hat I&rsquo;ll wear although, like I said this, this totally makes it look like this is my hair. Like on camera. I think this makes it look like I&rsquo;ve got big white hair.</p><p><strong>Charly Batista:</strong><br>Does that look like a wig?</p><p><strong>Matt Yonkovit:</strong><br>It does not look like a hat.? And Daniil was telling me like you got to get this hat for Peter. It&rsquo;s so awesome. It&rsquo;s so awesome. It&rsquo;s so awesome. And I got it. And now it looks like I&rsquo;m wearing a friggin wig. I mean, okay, thank you, Peter, for the hat. As I said, I love hats. I have a hat collection. I&rsquo;ve got like 1000s in the house. But this totally looks like a wig. When I look at it up to the camera. What does everybody else think? Comment? Does this look like a wig or a hat? I think it looks like a wig. And it probably has more hair than I have right now. Anyway, so maybe I&rsquo;ll just wear this for the rest of my days and people will just think I&rsquo;m cool.</p><p><strong>Charly Batista:</strong><br>Well, and it&rsquo;s probably quite warm and comfortable during winter, right?</p><p><strong>Matt Yonkovit:</strong><br>Yeah, unfortunately, it doesn&rsquo;t get very cold here. Yes, Daniil. You saw the hat. Yes. Wonderful. It doesn&rsquo;t look like a hat. It looks like a wig. Like I&rsquo;m like Mozart or something back in the day when they were the white wigs. Or I could Yeah, I don&rsquo;t know. But okay, so here&rsquo;s what&rsquo;s coming up. So next week, Charly is back with us at our regular scheduled time. As I said, this is not a regularly scheduled live stream. So if you see this, and you&rsquo;re like, What the heck did I miss it? No, you didn&rsquo;t miss it. We just didn&rsquo;t finish last week. So we wanted to come back, cover a few different things. Next week. What we&rsquo;re going to do is we&rsquo;re going to take the same instances, and Charly is going to come along and configure the alerts. And we&rsquo;re going to talk about what sort of things we should be monitoring in a production system. And what are sane thresholds for those production systems?</p><p><strong>Charly Batista:</strong><br>Yep. Yep. We got to start them.</p><p>Wait, wait, Daniil. It&rsquo;s not gray. It&rsquo;s white. Why would I look like a gray-haired Marge Simpson? No white. This is white. It&rsquo;s not gray, maybe a white-haired Marge Simpson, but I think it needs to go like this. Hi. Yeah. And it goes like, a lot. Yeah, yeah. Yeah. Anyway, so yes, Charly, next week. Alerts. Alerts. Yeah. Be prepared. Be prepared.</p><p>I&rsquo;ll keep an alert about next week. Okay.</p><p><strong>Matt Yonkovit:</strong><br>Yes. Great Daniil. You look forward to alerts, and maybe I&rsquo;ll wear this hat for alerts will alert you to this hat coming. All right. All right. Fair enough. And so for the two Grumpy Old Men, Chino, and the HOSS, we thank you very much for showing up today and watching the stream, and tune in next week for the discussion on alerting. Whoo. Alerting everybody. All right.</p><p><strong>Charly Batista:</strong><br>Keep eyes on it.</p><p><strong>Matt Yonkovit:</strong><br>Yes, yes. Thanks a bunch <span aria-label="End of article." role=text class=tombstone>âˆŽ</span></p></div><div class=bios><div class=bios__bio><img class=avatar src=https://percona.community/speakers/charly_batista_hu68a474ae9c5dbcdc11adae0aa6d4e7b8_6050_240x240_fill_q90_lanczos_smart1.jpg alt><div class=bios__content><h2><span>Charly Batista</span></h2><i>Percona, PostgreSQL Tech Lead</i><p>Charly Batista is currently PostgreSQL Tech Lead at Percona.
Possesses over twelve (12) years of experience in various areas of IT including Database Administration, Data Analysis, Systems Analysis and Software Development. Strong analytical skills combined with experience in object oriented programming techniques. Technical Leader for more than four (4) years for the Developer Team. Born in Brazil and now living in Shanghai-China.</p><p><a href=/speakers/charly_batista>See all talks by Charly Batista &#187;</a></p></div></div><div class=bios__bio><img class=avatar src=https://percona.community/speakers/matt_hu08bf5a6325671510ec52d6b168d6e7ee_55586_240x240_fill_q90_lanczos_smart1.jpg alt><div class=bios__content><h2><span>Matt Yonkovit</span></h2><i>The HOSS, Percona</i><p>Matt is currently working as the Head of Open Source Strategy (HOSS) for Percona, a leader in open source database software and services. He has over 15 years of experience in the open source industry including over 10 years of executive-level experience leading open source teams. Mattâ€™s experience merges the technical and business aspects of the open source database experience with both a passion for hands on development and management and the leadership of building strong teams. During his time he has created or managed business units responsible for service delivery ( consulting, support, and managed services ), customer success, product management, marketing, and operations. He currently leads efforts around Perconaâ€™s OSPO, community, and developer relations efforts. He hosts the HOSS talks FOSS podcast, writes regularly, and shares his MySQL and PostgreSQL knowledge as often as possible.</p><p><a href=/speakers/matt_yonkovit>See all talks by Matt Yonkovit &#187;</a></p></div></div></div></article><div class=sharing><a class=resp-sharing-button__link href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on Facebook"><div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg></div><span>Share on Facebook</span></div></a><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Observing%20%26%20Tuning%20Your%20PostgreSQL%20Workload%20Using%20PMM%20-%20Community%20PostgreSQL%20Live%20Stream%20%26%20Chat%20-%20March%2011th&url=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on Twitter"><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div><span>Share on Twitter</span></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f&title=Observing%20%26%20Tuning%20Your%20PostgreSQL%20Workload%20Using%20PMM%20-%20Community%20PostgreSQL%20Live%20Stream%20%26%20Chat%20-%20March%2011th&source=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on LinkedIn"><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6.0 2.5 1 2.6 2.5.0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/></svg></div><span>Share on LinkedIn</span></div></a><a class=resp-sharing-button__link href="https://telegram.me/share/url?text=Observing%20%26%20Tuning%20Your%20PostgreSQL%20Workload%20Using%20PMM%20-%20Community%20PostgreSQL%20Live%20Stream%20%26%20Chat%20-%20March%2011th&url=https%3a%2f%2fpercona.community%2fevents%2fstreams-pg%2f2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm%2f" target=_blank rel=noopener aria-label="Share on Telegram"><div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M.707 8.475C.275 8.64.0 9.508.0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102.0 001.75.527l2.96-2.41a.405.405.0 01.494-.013l5.34 3.87a1.1 1.1.0 001.046.135 1.1 1.1.0 00.682-.803l3.91-18.795A1.102 1.102.0 0022.5.075L.706 8.475z"/></svg></div><span>Share on Telegram</span></div></a></div><h2>Comments</h2><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://percona.community/events/streams-pg/2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm/"};(function(){var a=document,b=a.createElement('script');b.src='https://percona-community01.disqus.com/embed.js',b.setAttribute('data-timestamp',+new Date),(a.head||a.body).appendChild(b)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><div class=recommendations><h2>Did you like this post? Why not read more?</h2><div class="grid grid--3"><div class=grid__item><a href=/events/percona-community-live-2022/1-day/ class="post post--preview"><img src=/events/percona-community-live/days/Day-1_hue318526994aeba9c60dfc07215a3621f_1346731_682x384_fill_q90_lanczos_smart1.jpg alt><h3>Percona Community Live June 21 - Day 1</h3><div class=post__content><p>You are invited to participate in the first day of the Percona Community Live stream! Learn about VictoriaMetrics, flexible Indexing with Postgres, PostgreSQL backups, database resiliency, and many other things on June 21st. Join us on June 22nd and June 23d for more amazing content!</p><p class=readmore>Read more &#187;</p></div></a></div><div class=grid__item><a href=/events/streams-pg/2022-08-25-exploring-the-scalability-limits-of-postgresql-on-kubernetes/ class="post post--preview"><img src=/events/streams-pg/PG-Stream-Cover-Week-12-August-25_hu7cee2302829f12b0d2537b01ea0000a6_362852_682x384_fill_q90_lanczos_smart1.jpg alt><h3>Exploring the scalability limits of PostgreSQL on Kubernetes - Percona Community Live Stream August 25th</h3><div class=post__content><p></p><p class=readmore>Read more &#187;</p></div></a></div><div class=grid__item><a href=/events/streams-mysql/2022-09-02-designing-a-schema/ class="post post--preview"><img src=/events/streams-mysql/2022-09-02-stream-mysql-marcos-and-dave_huf2c89fee84d7f929d8f3ddf3e50ef2ad_318543_682x384_fill_q90_lanczos_smart1.jpg alt><h3>MySQL Designing a Schema - Percona Community MySQL Live Stream & Chat - Sept 2nd</h3><div class=post__content><p></p><p class=readmore>Read more &#187;</p></div></a></div></div></div></div></main><div class=edit-on-github><div class=container><a href=https://github.com/percona/community/edit/main/content/events/streams-pg/2022-03-11-observing-and-tuning-your-postgresql-workload-using-pmm.md target=_blank rel="noreferrer noopener">âœŽ Edit this page on GitHub</a></div></div><footer class=page><p class=aria-only id=footer-heading aria-hidden=false>Footer</p><div class=page-footer><nav class=footer><ul><li><a href=https://www.percona.com/about-percona target=_blank rel="noopener noreferrer"><span>About</span></a></li><li><a href=https://www.percona.com/legal target=_blank rel="noopener noreferrer"><span>Legal</span></a></li><li><a href=https://www.percona.com/about-percona/contact target=_blank rel="noopener noreferrer"><span>Contact</span></a></li><li><a href=https://forums.percona.com target=_blank rel="noopener noreferrer"><span>Forums</span></a></li><li><a href=http://per.co.na/discord target=_blank rel="noopener noreferrer"><span>Discord</span></a></li><li><a href=/map><span>Sitemap</span></a></li><li><a class=cmp-revoke-consent>Revoke Consent</a></li></ul></nav></div><aside>Â© Percona Community. MySQL, InnoDB, MariaDB and MongoDB are trademarks of their respective owners.</aside></footer><script type=text/javascript src=https://percona.community/js/aria.5f30892348d4662fe548bb410f15fc6fe2e1cbfd6473beb3c3ed1d6b362568bbea48896a9f2302a4f78f4614904ca3ef43155ae26f063ed84ae957f951e19647.js integrity="sha512-XzCJI0jUZi/lSLtBDxX8b+Lhy/1kc76zw+0dazYlaLvqSIlqnyMCpPePRhSQTKPvQxVa4m8GPthK6Vf5UeGWRw==" defer></script></body></html>